{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["SVN approche\n","Ce book integre:\n","    la recolte d'informations sur les commentaires\n","    la preparation des donn√©es pour etre trait√©s par liblinear\n","    la traitement de la sorti de liblinear"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from elasticsearch import Elasticsearch, helpers\n","import pickle\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from tqdm.gui import tqdm as tqdm_gui\n"]},{"cell_type":"markdown","metadata":{},"source":["AJout des functions d'import des data"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def xmlToDf(xmlFile):\n","    # Read XML file\n","    df = pd.read_xml(xmlFile)\n","    # replace comma to point in note column\n","    df[\"note\"] = df[\"note\"].apply(lambda x: x.replace(\",\", \".\"))\n","    df[\"commentaire\"] = df[\"commentaire\"].apply(checkIfWordInComment)\n","    # string to double conversion column\n","    df['note'] = df['note'].astype(float)\n","    return df\n","\n","def checkIfWordInComment(comment):\n","    if comment is None:\n","        return \"\"\n","    return comment\n","\n","def getNbUpperCases(comment):\n","    if comment is None:\n","        return 0\n","    return sum(1 for c in comment if c.isupper())\n","\n","def getSizeString(comment):\n","    if comment is None:\n","        return 0\n","    return int(len(comment))\n","\n","def getCountWords(comment):\n","    if comment is None:\n","        return 0\n","    return int(len(comment.split()))\n","\n","def getCountExclamation(comment):\n","    if comment is None:\n","        return 0\n","    return int(comment.count('!'))\n","\n","def getCountQuestion(comment):\n","    if comment is None:\n","        return 0\n","    return int(comment.count('?'))\n","\n","def getCountPoint(comment):\n","    if comment is None:\n","        return 0\n","    return int(comment.count('.'))\n","    \n","def getCountSmiley(comment):\n","    if comment is None:\n","        return 0\n","    return int(comment.count(':)'or ':(' or ':D' or 'xD'))\n","\n","def getCountPas(comment):\n","    if comment is None:\n","        return 0\n","    return int(comment.lower().count('pas'))\n","\n","def getCountGoodList(comment):\n","    if comment is None:\n","        return 0\n","    count = 0\n","    for word in good_list:\n","        count += comment.lower().count(word)\n","    return count\n","\n","def getCountBadList(comment):\n","    if comment is None:\n","        return 0\n","    count = 0\n","    for word in bad_list:\n","        count += comment.lower().count(word)\n","    return count\n","\n","def getClassRate(note):\n","    if note < 2:\n","        return 0\n","    elif note > 2:\n","        return 2\n","    else:\n","        return 1\n","\n","def loadGoodList():\n","    with open('../Projet/dict/good.txt', 'r') as f:\n","        return f.read().splitlines()\n","\n","def loadBadList():\n","    with open('../Projet/dict/bad.txt', 'r') as f:\n","        return f.read().splitlines()\n","\n","def loadAndGetBasicInfo(path):\n","    df_dev = xmlToDf(path)\n","    df_dev['class_rate'] = df_dev['note'].apply(getClassRate)    \n","    df_dev['size_char'] = df_dev['commentaire'].apply(getSizeString)\n","    print(\"Got size_char\")\n","    df_dev['uppercase'] = df_dev['commentaire'].apply(getNbUpperCases)\n","    print('Got uppercase')\n","    df_dev['ratio'] = df_dev['size_char']/df_dev['uppercase']\n","    print('Got ratio')\n","    df_dev['count_words'] = df_dev['commentaire'].apply(getCountWords)\n","    print('Got count_words')\n","    df_dev['count_exclamation'] = df_dev['commentaire'].apply(getCountExclamation)\n","    print('Got count_exclamation')\n","    df_dev['count_question'] = df_dev['commentaire'].apply(getCountQuestion)\n","    print('Got count_question')\n","    df_dev['count_point'] = df_dev['commentaire'].apply(getCountPoint)\n","    print('Got count_point')\n","    df_dev['count_smiley'] = df_dev['commentaire'].apply(getCountSmiley)\n","    print('Got count_smiley')\n","    df_dev['count_pas'] = df_dev['commentaire'].apply(getCountPas)\n","    print('Got count_pas')    \n","    tqdm.pandas(desc=\"Counting bad words\")\n","    df_dev['count_mal'] = df_dev['commentaire'].progress_apply(lambda x: getCountBadList(x))\n","    print('Got count_mal')    \n","    tqdm.pandas(desc=\"Counting good words\")\n","    df_dev['count_good'] = df_dev['commentaire'].progress_apply(lambda x: getCountGoodList(x))\n","    print('Got count_mal')\n","    return df_dev\n","\n","bad_list = loadBadList()\n","good_list = loadGoodList()"]},{"cell_type":"markdown","metadata":{},"source":["Import des data dev\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\wiakx\\OneDrive\\1Documents\\Cours\\M2\\INOV2\\Projet\\Projet.ipynb Cellule 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_dev \u001b[39m=\u001b[39m loadAndGetBasicInfo(\u001b[39m\"\u001b[39;49m\u001b[39mdata/dev.xml\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m (df_dev\u001b[39m.\u001b[39mkeys())\n","\u001b[1;32mc:\\Users\\wiakx\\OneDrive\\1Documents\\Cours\\M2\\INOV2\\Projet\\Projet.ipynb Cellule 5\u001b[0m in \u001b[0;36mloadAndGetBasicInfo\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloadAndGetBasicInfo\u001b[39m(path):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     df_dev \u001b[39m=\u001b[39m xmlToDf(path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     df_dev[\u001b[39m'\u001b[39m\u001b[39mclass_rate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_dev[\u001b[39m'\u001b[39m\u001b[39mnote\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(getClassRate)    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     df_dev[\u001b[39m'\u001b[39m\u001b[39msize_char\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_dev[\u001b[39m'\u001b[39m\u001b[39mcommentaire\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(getSizeString)\n","\u001b[1;32mc:\\Users\\wiakx\\OneDrive\\1Documents\\Cours\\M2\\INOV2\\Projet\\Projet.ipynb Cellule 5\u001b[0m in \u001b[0;36mxmlToDf\u001b[1;34m(xmlFile)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mxmlToDf\u001b[39m(xmlFile):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Read XML file\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_xml(xmlFile)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# replace comma to point in note column\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mnote\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mnote\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m))\n","File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\xml.py:938\u001b[0m, in \u001b[0;36mread_xml\u001b[1;34m(path_or_buffer, xpath, namespaces, elems_only, attrs_only, names, encoding, parser, stylesheet, compression, storage_options)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(\n\u001b[0;32m    739\u001b[0m     version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mpath_or_buffer\u001b[39m\u001b[39m\"\u001b[39m], stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m    740\u001b[0m )\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    757\u001b[0m     storage_options: StorageOptions \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    758\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m    759\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[39m    Read XML document into a ``DataFrame`` object.\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[39m    2  triangle      180    3.0\u001b[39;00m\n\u001b[0;32m    936\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 938\u001b[0m     \u001b[39mreturn\u001b[39;00m _parse(\n\u001b[0;32m    939\u001b[0m         path_or_buffer\u001b[39m=\u001b[39;49mpath_or_buffer,\n\u001b[0;32m    940\u001b[0m         xpath\u001b[39m=\u001b[39;49mxpath,\n\u001b[0;32m    941\u001b[0m         namespaces\u001b[39m=\u001b[39;49mnamespaces,\n\u001b[0;32m    942\u001b[0m         elems_only\u001b[39m=\u001b[39;49melems_only,\n\u001b[0;32m    943\u001b[0m         attrs_only\u001b[39m=\u001b[39;49mattrs_only,\n\u001b[0;32m    944\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    945\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    946\u001b[0m         parser\u001b[39m=\u001b[39;49mparser,\n\u001b[0;32m    947\u001b[0m         stylesheet\u001b[39m=\u001b[39;49mstylesheet,\n\u001b[0;32m    948\u001b[0m         compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    949\u001b[0m         storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    950\u001b[0m     )\n","File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\xml.py:733\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(path_or_buffer, xpath, namespaces, elems_only, attrs_only, names, encoding, parser, stylesheet, compression, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValues for parser can only be lxml or etree.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 733\u001b[0m data_dicts \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mparse_data()\n\u001b[0;32m    735\u001b[0m \u001b[39mreturn\u001b[39;00m _data_to_frame(data\u001b[39m=\u001b[39mdata_dicts, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\xml.py:389\u001b[0m, in \u001b[0;36m_LxmlFrameParser.parse_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39mParse xml data.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mand parse original or transformed XML and return specific nodes.\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlxml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39metree\u001b[39;00m \u001b[39mimport\u001b[39;00m XML\n\u001b[1;32m--> 389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxml_doc \u001b[39m=\u001b[39m XML(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_doc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath_or_buffer))\n\u001b[0;32m    391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstylesheet \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxsl_doc \u001b[39m=\u001b[39m XML(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_doc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstylesheet))\n","File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\xml.py:560\u001b[0m, in \u001b[0;36m_LxmlFrameParser._parse_doc\u001b[1;34m(self, raw_doc)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    558\u001b[0m         doc \u001b[39m=\u001b[39m parse(xml_data, parser\u001b[39m=\u001b[39mcurr_parser)\n\u001b[1;32m--> 560\u001b[0m \u001b[39mreturn\u001b[39;00m tostring(doc)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["df_dev = loadAndGetBasicInfo(\"data/dev.xml\")\n","print (df_dev.keys())"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Got size_char\n","Got uppercase\n","Got ratio\n","Got count_words\n","Got count_exclamation\n","Got count_question\n","Got count_point\n","Got count_smiley\n","Got count_pas\n"]},{"name":"stderr","output_type":"stream","text":["Counting bad words: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665962/665962 [00:16<00:00, 40415.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Got count_mal\n"]},{"name":"stderr","output_type":"stream","text":["Counting good words: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665962/665962 [00:16<00:00, 40445.88it/s]"]},{"name":"stdout","output_type":"stream","text":["Got count_mal\n","Index(['movie', 'review_id', 'name', 'user_id', 'note', 'commentaire',\n","       'class_rate', 'size_char', 'uppercase', 'ratio', 'count_words',\n","       'count_exclamation', 'count_question', 'count_point', 'count_smiley',\n","       'count_pas', 'count_mal', 'count_good'],\n","      dtype='object')\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["df_train = loadAndGetBasicInfo(\"data/train.xml\")\n","print (df_train.keys())"]},{"cell_type":"markdown","metadata":{},"source":["Import des data via deserialize"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def deserializeDf(path):\n","    with open(path, 'rb') as f:\n","        return pickle.load(f)\n","    \n","# df_dev = deserializeDf('data/df_dev.pkl')\n","# df_idf = deserializeDf('data/df_idf.pkl')\n","df_train = deserializeDf('data/df_train.pkl')\n","#df_idf_train = deserializeDf('data/df_idf_train.pkl')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["stop_words = [\" \", \"l'\", \"l‚Äô\", \"la\", \"le\", \"les\", \"d‚Äô\", \"d'\", \"de\", \"du\", \"des\", \"une\", \"un\",\n","                \"ce\", \"ces\", \"je\", \"moi\", \"mon\", \"me\", \"mes\", \"tu\", \"toi\", \"ton\", \"te\", \"tes\", \n","                \"il\", \"lui\", \"son\", \"se\", \"ses\", \"nous\", \"notre\", \"nos\", \"vous\", \"votre\", \"vos\",\n","                \"ils\", \"leur\", \"leurs\", \"n'\", \"ne\", \"tout\", \"√™tre\", \"avoir\", \"deja\", \"d√©j√†\",\n","                \"ou\" ,\"o√π\", \"qu‚Äô\", \"qu'\", \"que\", \"qui\", \"quelle\", \"quel\", \"quelles\", \"quels\", \n","                \".\", \",\", \"...\", \"sur\", \"telle\", \"tel\", \"telles\", \"tels\", \"laquelle\", \"lequel\",\n","                \"laquelles\", \"lequels\", \"simplement\", \"comment\", \"quoi\", \"dont\", \"donc\", \"tant\",\n","                \"jamais\", \"rarement\", \"parfois\", \"souvent\", \"toujours\", \"avec\", \"pour\", \"ici\",\n","                \":\", \"(\", \")\", \"[\", \"]\", \"\\\"\", \"y\", \"et\", \"par\", \"fois\", \"peu\", \"on\", \"cela\",\n","                \"mais\", \"dans\", \"en\", \"√†\", \"au\", \"m√™me\", \"l√†\", \"-\", \"si\", \"comme\", \"aussi\",\n","                \"car\", \"parce\", \"quand\"]"]},{"cell_type":"markdown","metadata":{},"source":["Separation de la data en 3 classes (0.5-1.5 / 2 / 2.5-5)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["df_low = df_dev.loc[df_dev['note'] < 2]\n","df_mid = df_dev.loc[(df_dev['note'] == 2)]\n","df_high = df_dev.loc[df_dev['note'] > 2]\n","df_dev['class_rate'] = df_dev['note'].apply(getClassRate) "]},{"cell_type":"markdown","metadata":{},"source":["Des graphs"]},{"cell_type":"markdown","metadata":{},"source":["Normalisation + stemming"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  (0, 19757)\t0.18687771683382542\n","  (0, 48901)\t0.19070082913350986\n","  (0, 83973)\t0.03842537283589266\n","  (0, 19314)\t0.12576934176003657\n","  (0, 88498)\t0.10989720432141713\n","  (0, 32824)\t0.07478572548905398\n","  (0, 61769)\t0.07723367066740257\n","  (0, 71022)\t0.135551918556739\n","  (0, 79063)\t0.09634334825587375\n","  (0, 68853)\t0.031621853764034064\n","  (0, 90127)\t0.05416056160163361\n","  (0, 58850)\t0.08835171409701363\n","  (0, 71831)\t0.11924719046445877\n","  (0, 92322)\t0.02717659835194249\n","  (0, 88448)\t0.08192120620741246\n","  (0, 92287)\t0.022038799711691436\n","  (0, 46918)\t0.127714871290098\n","  (0, 76354)\t0.1209863061717123\n","  (0, 66526)\t0.05101464405073643\n","  (0, 63071)\t0.030983157942342035\n","  (0, 47014)\t0.18687771683382542\n","  (0, 97566)\t0.1209132443451202\n","  (0, 64725)\t0.09564533630715794\n","  (0, 33424)\t0.15676038569827785\n","  (0, 21448)\t0.09448442761009675\n","  :\t:\n","  (100399, 72074)\t0.061359460769206585\n","  (100399, 2636)\t0.07123186957143746\n","  (100399, 8753)\t0.05516281640257527\n","  (100399, 68853)\t0.10107447111897357\n","  (100399, 92322)\t0.04343294238428818\n","  (100399, 66526)\t0.08153029555489726\n","  (100399, 63071)\t0.04951648828032435\n","  (100399, 53031)\t0.04240743594617035\n","  (100399, 66481)\t0.11724288290557584\n","  (100399, 87946)\t0.10708871123319345\n","  (100399, 37467)\t0.035875985290216475\n","  (100399, 36339)\t0.06701737438266496\n","  (100399, 86373)\t0.1244028304261439\n","  (100399, 34752)\t0.10249310200257773\n","  (100399, 52026)\t0.11665400531155283\n","  (100399, 72309)\t0.045935743025608454\n","  (100399, 37445)\t0.12827984469748246\n","  (100399, 24575)\t0.04791243190205182\n","  (100399, 23781)\t0.13359807998561313\n","  (100399, 32443)\t0.045728160689779966\n","  (100399, 72219)\t0.09685035178573383\n","  (100399, 52778)\t0.14691417718040642\n","  (100399, 65134)\t0.04457643225115032\n","  (100399, 34622)\t0.22490861817593397\n","  (100399, 15624)\t0.04675394240755744\n"]}],"source":["def TfIdf(df, column):\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    vectorizer = TfidfVectorizer()\n","    X = vectorizer.fit_transform(df[column])\n","    return X\n","\n","x = TfIdf(df_dev, 'commentaire')\n","print(x)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idf_weights</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>de</th>\n","      <td>1.219785</td>\n","    </tr>\n","    <tr>\n","      <th>et</th>\n","      <td>1.247718</td>\n","    </tr>\n","    <tr>\n","      <th>un</th>\n","      <td>1.286338</td>\n","    </tr>\n","    <tr>\n","      <th>film</th>\n","      <td>1.310228</td>\n","    </tr>\n","    <tr>\n","      <th>le</th>\n","      <td>1.341365</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>inerent</th>\n","      <td>11.823780</td>\n","    </tr>\n","    <tr>\n","      <th>inerpr√©t√©</th>\n","      <td>11.823780</td>\n","    </tr>\n","    <tr>\n","      <th>inerpr√©t√©e</th>\n","      <td>11.823780</td>\n","    </tr>\n","    <tr>\n","      <th>ind√©tron√¢ble</th>\n","      <td>11.823780</td>\n","    </tr>\n","    <tr>\n","      <th>ÈªÑÊµ∑</th>\n","      <td>11.823780</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>99180 rows √ó 1 columns</p>\n","</div>"],"text/plain":["              idf_weights\n","de               1.219785\n","et               1.247718\n","un               1.286338\n","film             1.310228\n","le               1.341365\n","...                   ...\n","inerent         11.823780\n","inerpr√©t√©       11.823780\n","inerpr√©t√©e      11.823780\n","ind√©tron√¢ble    11.823780\n","ÈªÑÊµ∑              11.823780\n","\n","[99180 rows x 1 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#instantiate CountVectorizer() \n","cv=CountVectorizer() \n","# this steps generates word counts for the words in your docs \n","\n","word_count_vector=cv.fit_transform(df_dev['commentaire'])\n","tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n","tfidf_transformer.fit(word_count_vector)\n","# print idf values \n","df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(),columns=[\"idf_weights\"]) \n","# sort ascending \n","df_idf.sort_values(by=['idf_weights'])"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idf_weights</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>de</th>\n","      <td>1.217917</td>\n","    </tr>\n","    <tr>\n","      <th>et</th>\n","      <td>1.244742</td>\n","    </tr>\n","    <tr>\n","      <th>un</th>\n","      <td>1.279303</td>\n","    </tr>\n","    <tr>\n","      <th>film</th>\n","      <td>1.308810</td>\n","    </tr>\n","    <tr>\n","      <th>le</th>\n","      <td>1.338168</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>ilustr√©es</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>il√©t√©o√πheinoussama</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>im6</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>imersif</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>ùôëùôäùôé</th>\n","      <td>13.715842</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>238779 rows √ó 1 columns</p>\n","</div>"],"text/plain":["                    idf_weights\n","de                     1.217917\n","et                     1.244742\n","un                     1.279303\n","film                   1.308810\n","le                     1.338168\n","...                         ...\n","ilustr√©es             13.715842\n","il√©t√©o√πheinoussama    13.715842\n","im6                   13.715842\n","imersif               13.715842\n","ùôëùôäùôé                   13.715842\n","\n","[238779 rows x 1 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#instantiate CountVectorizer() \n","cv=CountVectorizer() \n","# this steps generates word counts for the words in your docs \n","\n","word_count_vector=cv.fit_transform(df_train['commentaire'])\n","tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n","tfidf_transformer.fit(word_count_vector)\n","# print idf values \n","df_idf_train = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(),columns=[\"idf_weights\"]) \n","# sort ascending \n","df_idf_train.sort_values(by=['idf_weights'])"]},{"cell_type":"markdown","metadata":{},"source":["trie des stopwords + Stemming des mots"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\wiakx\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["['on', 'transforme', 'le', 'liste', 'de', 'sentiment', 'en', 'tenseur']\n","['transforme', 'liste', 'sentiment', 'tenseur']\n"]}],"source":["import spacy\n","import nltk\n","import multiprocessing as mp\n","\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","stemmer = SnowballStemmer(language='french')\n","nltk.download('stopwords')\n","stopWords = set(stopwords.words('french'))\n","nlp = spacy.load(\"fr_core_news_sm\")\n","\n","def getStemWord(comment):\n","    if comment is None:\n","        return \"\"\n","    doc = nlp(comment)\n","    tokens = [X.text for X in doc]\n","    clean_words = []\n","    for token in tokens:\n","        if token not in stopWords:\n","            clean_words.append(token)\n","    return [stemmer.stem(token) for token in clean_words]\n","\n","def getProperName(comment):\n","    if comment is None:\n","        return \"\"\n","    doc = nlp(comment)\n","    return [(X.text, X.label_) for X in doc.ents]\n","\n","\n","tqdm.pandas(desc=\"Stemming words\")\n","df_train['stem_word'] = df_train['commentaire'].progress_apply(lambda x: getStemWord(x))\n","df_train['proper_name'] = df_train['commentaire'].progress_apply(lambda x: getProperName(x))\n","# df_dev['stem_word'] = df_dev['commentaire'].progress_apply(lambda x: getStemWord(x))\n","# df_dev['proper_name'] = df_dev['commentaire'].progress_apply(lambda x: getProperName(x))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\wiakx\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["on commence\n"]},{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100400/100400 [1:35:48<00:00, 17.47it/s] \n"]}],"source":["import spacy\n","import nltk\n","import multiprocessing as mp\n","import concurrent.futures\n","\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","# stopWords = set(stopwords.words('french'))\n","# nlp = spacy.load(\"fr_core_news_sm\") # more efficient but less accurate\n","nlp = spacy.load(\"fr_dep_news_trf\") # less efficient but more accurate\n","\n","def getLemWord(comment):\n","    if comment is None:\n","        return \"\"\n","    doc = nlp(comment)\n","    tokens = [X.lemma_ for X in doc]\n","    clean_words = []\n","    for token in tokens:\n","        if token not in stop_words:\n","            tokens.remove(token)\n","    return tokens\n","\n","\n","\n","tqdm.pandas(desc=\"Lemma words\")\n","df_train['lemma_word'] = df_train['commentaire'].progress_apply(lambda x: getLemWord(x))\n","# df_dev['lemma_word'] = df_dev['commentaire'].progress_apply(lambda x: getLemWord(x))\n","# df_dev['proper_name'] = df_dev['commentaire'].progress_apply(lambda x: getProperName(x))\n","\n","comments = df_dev['commentaire'].tolist()\n","print('on commence')\n","with concurrent.futures.ThreadPoolExecutor(max_workers=mp.cpu_count()) as executor:\n","    results = list(tqdm(executor.map(getLemWord, comments), total=len(comments)))\n","\n","\n","df_dev['lemma_word'] = results"]},{"cell_type":"markdown","metadata":{},"source":["Redivision des donn√©es en classe selon les notes"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------------\n","Low\n","\n",", 65003\n",". 53775\n","film 23133\n","! 15705\n","d' 15523\n","l' 15518\n","a 13247\n","\" 11370\n","c' 9739\n","... 9619\n","-----------------\n","Mid\n","\n",", 42438\n",". 34898\n","film 13736\n","l' 10629\n","d' 9512\n","a 7670\n","\" 7034\n","plus 5683\n","tout 5548\n","! 5457\n","-----------------\n","High\n","\n",", 375462\n",". 320961\n","film 136172\n","l' 100379\n","d' 88764\n","! 67648\n","a 63611\n","\" 55377\n","bien 50641\n","tout 50013\n","-----------------\n","End\n"]}],"source":["df_low = df_dev.loc[df_dev['note'] < 2]\n","df_mid = df_dev.loc[(df_dev['note'] == 2)]\n","df_high = df_dev.loc[df_dev['note'] > 2]\n","\n","\n","def dictFreqEachWords(df):\n","    dictFreq = {}\n","    for index, row in df.iterrows():\n","        for word in row['stem_word']:\n","            if word in dictFreq:\n","                dictFreq[word] += 1\n","            else:\n","                dictFreq[word] = 1\n","    return dictFreq\n","\n","def OrderDictByValue(dictFreq):\n","    return {k: v for k, v in sorted(dictFreq.items(), key=lambda item: item[1], reverse=True)}\n","\n","def PrintXFirstDict(dictFreq, x):\n","    for key, value in dictFreq.items():\n","        if x > 0:\n","            print(key, value)\n","            x -= 1\n","        else:\n","            break\n","    return\n","\n","dict_low = dictFreqEachWords(df_low)\n","dict_mid = dictFreqEachWords(df_mid)\n","dict_high = dictFreqEachWords(df_high)\n","dict_low = OrderDictByValue(dict_low)\n","dict_mid = OrderDictByValue(dict_mid)\n","dict_high = OrderDictByValue(dict_high)\n","\n","print('-----------------')\n","print('Low\\n')\n","PrintXFirstDict(dict_low, 10)\n","print('-----------------')\n","print('Mid\\n')\n","PrintXFirstDict(dict_mid, 10)\n","print('-----------------')\n","print('High\\n')\n","PrintXFirstDict(dict_high, 10)\n","\n","print('-----------------')\n","print('End')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'dict_high' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m             dict_low\u001b[39m.\u001b[39mpop(listWord[i])\n\u001b[0;32m      7\u001b[0m             dict_mid\u001b[39m.\u001b[39mpop(listWord[i])\n\u001b[1;32m----> 9\u001b[0m removeSimilarInDicts()\n","Cell \u001b[1;32mIn[11], line 2\u001b[0m, in \u001b[0;36mremoveSimilarInDicts\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremoveSimilarInDicts\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     listWord \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(dict_high\u001b[39m.\u001b[39mkeys())[:\u001b[39m30\u001b[39m]\n\u001b[0;32m      3\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m30\u001b[39m):\n\u001b[0;32m      4\u001b[0m         \u001b[39mif\u001b[39;00m listWord[i] \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(dict_mid\u001b[39m.\u001b[39mkeys())[:\u001b[39m30\u001b[39m] \u001b[39mor\u001b[39;00m listWord[i] \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(dict_low\u001b[39m.\u001b[39mkeys())[:\u001b[39m30\u001b[39m]:\n","\u001b[1;31mNameError\u001b[0m: name 'dict_high' is not defined"]}],"source":["def removeSimilarInDicts():\n","    listWord = list(dict_high.keys())[:30]\n","    for i in range(0, 30):\n","        if listWord[i] in list(dict_mid.keys())[:30] or listWord[i] in list(dict_low.keys())[:30]:\n","            dict_high.pop(listWord[i])\n","            dict_low.pop(listWord[i])\n","            dict_mid.pop(listWord[i])\n","            \n","removeSimilarInDicts()"]},{"cell_type":"markdown","metadata":{},"source":["Serialize df into file (pour save)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def serializeDf(df, path):\n","    with open(path, 'wb') as f:\n","        pickle.dump(df, f)\n","        \n","serializeDf(df_dev, 'data/df_dev.pkl')\n","# serializeDf(df_idf, 'data/df_idf.pkl')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def serializeDf(df, path):\n","    with open(path, 'wb') as f:\n","        pickle.dump(df, f)\n","        \n","serializeDf(df_train, 'data/df_train.pkl')\n","#serializeDf(df_idf_train, 'data/df_idf_train.pkl')"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'df_dev' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\wiakx\\OneDrive\\1Documents\\Cours\\M2\\INOV2\\Projet\\Projet.ipynb Cellule 32\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             row[\u001b[39m'\u001b[39m\u001b[39mstem_word\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mremove(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m removeCommaInDF()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(df_dev[\u001b[39m'\u001b[39m\u001b[39mstem_word\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m10\u001b[39m])\n","\u001b[1;31mNameError\u001b[0m: name 'df_dev' is not defined"]}],"source":["def removeCommaInDF():\n","    for index, row in df_train.iterrows():\n","        if row['lemma_word'].count(',') > 0:\n","            row['lemma_word'].remove(',')\n","\n","removeCommaInDF()\n","print(df_train['lemma_word'][:10])"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def listToString(s):  \n","    str1 = \" \" \n","    return (str1.join(s))\n","\n","# df_train['stem_word_string'] = df_train['stem_word'].apply(lambda x: listToString(x))\n","df_train['lemma_word_string'] = df_train['lemma_word'].apply(lambda x: listToString(x))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idf_stem_weights</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>film</th>\n","      <td>1.276222</td>\n","    </tr>\n","    <tr>\n","      <th>bien</th>\n","      <td>1.971428</td>\n","    </tr>\n","    <tr>\n","      <th>tout</th>\n","      <td>2.002309</td>\n","    </tr>\n","    <tr>\n","      <th>tres</th>\n","      <td>2.011759</td>\n","    </tr>\n","    <tr>\n","      <th>bon</th>\n","      <td>2.025595</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>hypeee</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>hype</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>hypcondriaqu</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>hypercar</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>ùôëùôäùôé</th>\n","      <td>13.715842</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>148062 rows √ó 1 columns</p>\n","</div>"],"text/plain":["              idf_stem_weights\n","film                  1.276222\n","bien                  1.971428\n","tout                  2.002309\n","tres                  2.011759\n","bon                   2.025595\n","...                        ...\n","hypeee               13.715842\n","hype                 13.715842\n","hypcondriaqu         13.715842\n","hypercar             13.715842\n","ùôëùôäùôé                  13.715842\n","\n","[148062 rows x 1 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#instantiate CountVectorizer() \n","cv=CountVectorizer() \n","# this steps generates word counts for the words in your docs \n","\n","word_count_vector=cv.fit_transform(df_train['stem_word_string'])\n","tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n","tfidf_transformer.fit(word_count_vector)\n","# print idf values \n","df_idf_train = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(),columns=[\"idf_stem_weights\"]) \n","# sort ascending \n","df_idf_train.sort_values(by=['idf_stem_weights'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def serializeDf(df, path):\n","    with open(path, 'wb') as f:\n","        pickle.dump(df, f)\n","        \n","serializeDf(df_train, 'data/df_train.pkl')\n","serializeDf(df_idf_train, 'data/df_idf_train.pkl')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\sdk\\python10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Deletings useless words: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665962/665962 [22:24<00:00, 495.41it/s] \n"]}],"source":["from multiprocessing import Pool\n","from tqdm.contrib.concurrent import process_map\n","\n","def stemWordLow(stem_word):\n","    if stem_word is None:\n","        return []\n","    clean_words = []\n","    for token in stem_word:\n","        if token in df_idf_train.index and df_idf_train.loc[token]['idf_stem_weights'] < 10:\n","            clean_words.append(token)\n","    return clean_words\n","\n","def stemWordSuperLow(stem_word):\n","    if stem_word is None:\n","        return []\n","    clean_words = []\n","    for token in stem_word:\n","        if token in df_idf_train.index and df_idf_train.loc[token]['idf_stem_weights'] < 9:\n","            clean_words.append(token)\n","    return clean_words\n","\n","def removeUselessWords():\n","    df_train['stem_word_low'] = df_train['stem_word'].apply(lambda x: stemWordLow(x))\n","\n","def removeSuperUselessWords():\n","    tqdm.pandas(desc=\"Deletings useless words\")\n","    df_train['stem_word_super_low'] = df_train['stem_word'].progress_apply(lambda x: stemWordSuperLow(x))\n","\n","removeUselessWords()\n","# removeSuperUselessWords()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unexpected exception formatting exception. Falling back to standard exception\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"C:\\Users\\wiakx\\AppData\\Local\\Temp\\ipykernel_16692\\257940883.py\", line 9, in <module>\n","    tmp = getUniqueToken()\n","  File \"C:\\Users\\wiakx\\AppData\\Local\\Temp\\ipykernel_16692\\257940883.py\", line -1, in getUniqueToken\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2052, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n","    frames.append(self.format_record(r))\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n","    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 734, in lines\n","    pieces = self.included_pieces\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n","    pos = scope_pieces.index(self.executing_piece)\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n","    return only(\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\executing\\executing.py\", line 190, in only\n","    raise NotOneValueFound('Expected one value, found 0')\n","executing.executing.NotOneValueFound: Expected one value, found 0\n"]}],"source":["#get all unique token in df_train['stem_word_super_low']\n","def getUniqueToken():\n","    listToken = []\n","    for index, row in df_train.iterrows():\n","        for token in row['stem_word_super_low']:\n","            if token not in listToken:\n","                listToken.append(token)\n","    return listToken\n","tmp = getUniqueToken()\n","print(len(tmp))"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["def create_dict_of_words():\n","    word_to_id = {}\n","    index = 0\n","    # for comment in df_dev['stem_word_low']:\n","    for comment in df_train['stem_word_super_low']:\n","        for token in comment:\n","            if token not in word_to_id:\n","                word_to_id[token] = index\n","                index += 1\n","    return word_to_id\n","\n","def create_dict_of_bigrams():\n","    bigram_to_id = {}\n","    multiple_bigram_to_id = {}\n","    index = 0\n","    multiple_index = 0\n","    # for comment in df_dev['stem_word_low']:\n","    for comment in df_train['stem_word_super_low']:\n","        for i in range(len(comment) - 1):\n","            bigram = (comment[i], comment[i+1])\n","            if bigram not in bigram_to_id:\n","                bigram_to_id[bigram] = index\n","                index += 1\n","            else:\n","                if bigram not in multiple_bigram_to_id:\n","                    multiple_bigram_to_id[bigram] = multiple_index\n","                    multiple_index += 1\n","    return multiple_bigram_to_id\n","\n","word_to_id = create_dict_of_words()\n","bigram_to_id = create_dict_of_bigrams()\n"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["def comment_to_vec(comment):\n","    vector = [0] * len(word_to_id)\n","    for token in comment:\n","        vector[word_to_id[token]] += 1\n","    return vector\n","\n","def vector_to_svm(vector):\n","    svm = ''\n","    for i in range(len(vector)):\n","        if vector[i] > 0:\n","            svm += str(i+1) + ':' + str(vector[i]) + ' '\n","    return svm\n","\n","def comment_to_svm(comment, note):\n","    dict_comment = {}\n","    for token in comment:\n","        dict_comment[token] = dict_comment.get(token, 0) + 1\n","    dict_bi_comment = {}\n","    for i in range(len(comment) - 1):\n","        bigram = (comment[i], comment[i+1])\n","        dict_bi_comment[bigram] = dict_bi_comment.get(bigram, 0) + 1\n","    svm = '' + str(note*2) + ' '\n","    #sort by key with word_to_id\n","    # dict_comment = {k: v for k, v in sorted(dict_comment.items(), key=lambda item: word_to_id[item[0]])}\n","    # for token in dict_comment:\n","    #     svm += str(word_to_id[token]+1) + ':' + str(dict_comment[token]) + ' '\n","    \n","    tmp = {}\n","    #remove bigram from dict_bi_comment if it is in bigram_to_id\n","    for token in dict_bi_comment:\n","        if bigram_to_id.get(token) is not None:\n","            tmp[token] = dict_bi_comment[token]\n","    dict_bi_comment = tmp\n","    #sort by key with bigram_to_id\n","    dict_bi_comment = {k: v for k, v in sorted(dict_bi_comment.items(), key=lambda item: bigram_to_id[item[0]])}\n","    for token in dict_bi_comment:\n","        if bigram_to_id.get(token) is not None:\n","            svm += str(bigram_to_id[token]+1) + ':' + str(dict_bi_comment[token]) + ' '\n","            # svm += str(bigram_to_id[token]+1+len(word_to_id)) + ':' + str(dict_bi_comment[token]) + ' '\n","    return svm\n","    \n","\n","def comment_to_svm_class(comment, note):\n","    dict_comment = {}\n","    for token in comment:\n","        if (word_to_id.get(token) is not None):\n","            dict_comment[token] = dict_comment.get(token, 0) + 1\n","    if note < 2:\n","        svm = '' + str(1) + ' '\n","    elif note == 2:\n","        svm = '' + str(2) + ' '\n","    else:\n","        svm = '' + str(3) + ' '\n","    #sort by key with word_to_id\n","    dict_comment = {k: v for k, v in sorted(dict_comment.items(), key=lambda item: word_to_id[item[0]])}\n","    for token in dict_comment:\n","        svm += str(word_to_id[token]+1) + ':' + str(dict_comment[token]) + ' '\n","    return svm"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["comment_low_train = df_train['stem_word_super_low'].values.tolist()\n","note_train = df_train['note'].values.tolist()\n","del(df_train)\n","#del(df_idf_train)"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665962/665962 [01:43<00:00, 6440.59it/s]\n"]}],"source":["svms = []\n","for index, comment in enumerate(tqdm(comment_low_train)):\n","    svm = comment_to_svm(comment, note_train[index])\n","    svms.append(svm)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665962/665962 [00:36<00:00, 18009.08it/s]\n"]}],"source":["svms = []\n","for index, comment in enumerate(tqdm(comment_low_train)):\n","    svm = comment_to_svm_class(comment, note_train[index])\n","    svms.append(svm)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["# save svms in svm file\n","with open('data/SVM_TRAIN_10_only_bi.svm', 'w') as f:\n","    for svm in svms:\n","        f.write(svm + '\\n')"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100400/100400 [01:27<00:00, 1142.96it/s]\n"]}],"source":["svmsC = []\n","for i ,vector in enumerate(tqdm(vectors)):\n","    if(df_dev['note'][i] < 2):\n","        svm = str(1) + ' ' + vector_to_svm(vector)\n","    elif(df_dev['note'][i] == 2):\n","        svm = str(2) + ' ' + vector_to_svm(vector)\n","    else:\n","        svm = str(3) + ' ' + vector_to_svm(vector)\n","    svmsC.append(svm)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# save svms in svm file\n","with open('data/svmsT.svm', 'w') as f:\n","    for svm in svms:\n","        f.write(svm + '\\n')\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["    movie        review_id  name                   user_id  note  \\\n","0  229864  review_58145147  AM11  Z20051105131449940367618   4.0   \n","\n","                                         commentaire  class_rate  size_char  \\\n","0  \"Ce n'est pas ce qui se trouve en face de vous...           2       1186   \n","\n","   uppercase  ratio  ...  count_question  count_point  count_smiley  \\\n","0         10  118.6  ...               0            8             0   \n","\n","   count_pas  count_mal  count_good  \\\n","0          4          0           3   \n","\n","                                           stem_word  \\\n","0  [\", ce, n', trouv, fac, c', tient, c√¥t, \", ., ...   \n","\n","                proper_name  \\\n","0  [(Joseph Kosinski, PER)]   \n","\n","                                    stem_word_string  \\\n","0  \" ce n' trouv fac c' tient c√¥t \" . cet phras t...   \n","\n","                                       stem_word_low  \n","0  [ce, trouv, fac, tient, c√¥t, cet, phras, tagli...  \n","\n","[1 rows x 22 columns]\n"]}],"source":["print(df_train.head(1))"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Pour les tests</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_test = deserializeDf('data/df_test.pkl')"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\wiakx\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85847/85847 [00:08<00:00, 10005.81it/s]\n"]}],"source":["import spacy\n","import nltk\n","import multiprocessing as mp\n","\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","stemmer = SnowballStemmer(language='french')\n","nltk.download('stopwords')\n","stopWords = set(stopwords.words('french'))\n","nlp = spacy.load(\"fr_core_news_sm\")\n","\n","def getStemWord(comment):\n","    if comment is None:\n","        return \"\"\n","    doc = nlp(comment)\n","    tokens = [X.text for X in doc]\n","    clean_words = []\n","    for token in tokens:\n","        if token not in stopWords:\n","            clean_words.append(token)\n","    return [stemmer.stem(token) for token in clean_words]\n","\n","def xmlToDfTest(xmlFile):\n","    # Read XML file\n","    df = pd.read_xml(xmlFile)\n","    df[\"commentaire\"] = df[\"commentaire\"].apply(checkIfWordInComment)\n","    return df\n","\n","def checkIfWordInComment(comment):\n","    if comment is None:\n","        return \"\"\n","    return comment\n","\n","def stemWordLow(stem_word):\n","    if stem_word is None:\n","        return []\n","    clean_words = []\n","    for token in stem_word:\n","        if token in df_idf_train.index and df_idf_train.loc[token]['idf_stem_weights'] < 9:\n","            clean_words.append(token)\n","    return clean_words\n","\n","def comment_to_svm_test(comment):\n","    dict_comment = {}\n","    for token in comment:\n","        if (word_to_id.get(token) is not None):\n","            dict_comment[token] = dict_comment.get(token, 0) + 1\n","    dict_bi_comment = {}\n","    for i in range(len(comment)-1):\n","        token = comment[i] + ' ' + comment[i+1]\n","        if (bigram_to_id.get(token) is not None):\n","            dict_bi_comment[token] = dict_bi_comment.get(token, 0) + 1\n","    svm = '1 '\n","    #sort by key with word_to_id\n","    dict_comment = {k: v for k, v in sorted(dict_comment.items(), key=lambda item: word_to_id[item[0]])}\n","    # for token in dict_comment:\n","    #     svm += str(word_to_id[token]+1) + ':' + str(dict_comment[token]) + ' '\n","    \n","    tmp = {}\n","    #remove bigram from dict_bi_comment if it is not in bigram_to_id\n","    for token in dict_bi_comment:\n","        if bigram_to_id.get(token) is not None:\n","            tmp[token] = dict_bi_comment[token]\n","    dict_bi_comment = tmp\n","\n","    dict_bi_comment = {k: v for k, v in sorted(dict_bi_comment.items(), key=lambda item: bigram_to_id[item[0]])}\n","    for token in dict_bi_comment:\n","        svm += str(bigram_to_id[token]+1) + ':' + str(dict_bi_comment[token]) + ' '\n","        # svm += str(bigram_to_id[token]+1+len(word_to_id)) + ':' + str(dict_bi_comment[token]) + ' '\n","    return svm\n","\n","#df_test = xmlToDfTest('data/test.xml')\n","#tqdm.pandas(desc=\"Stemming\")\n","#df_test['stem_word'] = df_test['commentaire'].progress_apply(lambda x: getStemWord(x))\n","\n","comment_test = df_test['stem_word'].values.tolist()\n","\n","svm_test = []\n","for index, comment in enumerate(tqdm(comment_test)):\n","    svm = comment_to_svm_test(comment)\n","    svm_test.append(svm)\n","\n","\n","# save svms in svm file\n","with open('data/svm_test_only_bi.svm', 'w') as f:\n","    for svm in svm_test:\n","        f.write(svm + '\\n')\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["serializeDf(df_test, 'data/df_test.pkl')"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["result = []\n","with open('data/output10_low_bi_v2.txt', 'r') as f:\n","    result = f.readlines()\n","r = []\n","for i in range(len(result)):\n","    tmp = df_test['review_id'][i] + ' ' + str(float(result[i])/2)\n","    tmp = tmp.replace('\\n', '').replace('.', ',')\n","    r.append(tmp)\n","\n","with open('data/result10_low_bi_v2.txt', 'w') as f:\n","    for i in range(len(r)):\n","        f.write(r[i] + '\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["result = []\n","with open('data/output10.txt', 'r') as f:\n","    result = f.readlines()\n","r = []\n","for i in range(len(result)):\n","    tmp = df_test['review_id'][i] + ' ' + str(float(result[i])/2)\n","    tmp = tmp.replace('\\n', '').replace('.', ',')\n","    r.append(tmp)\n","\n","with open('data/result10.txt', 'w') as f:\n","    for i in range(len(r)):\n","        f.write(r[i] + '\\n')\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["    movie        review_id              name                   user_id  \\\n","0  253727  review_59354742  gimliamideselfes  Z20040827093310567684711   \n","1  253727  review_62570109            Yetcha  Z20030318104639813779116   \n","2  253727  review_58180650          TTNOUGAT  Z20071021153249553451578   \n","3  253727  review_57696986       titicaca120  Z20060317115155370578365   \n","4  253727  review_57736972           velocio  Z20040630141522227308769   \n","\n","                                         commentaire  \n","0  12 jours, un film que j'ai rat√© √† sa sortie et...  \n","1  Ces lueurs dans ces yeux... Soit ils sont tous...  \n","2  Il s‚Äôagit d‚Äôun documentaire sans la moindre ac...  \n","3  un documentaire magnifique dans cette unit√© ly...  \n","4  Un documentaire tr√®s int√©ressant, quand bien m...  \n"]}],"source":["df_test = xmlToDfTest('data/test.xml')\n","print(df_test.head())"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"54db34dbb873d0124069a1b7e3692f2fcb3af91d00ed8e76b38ddecc02ef7a27"}}},"nbformat":4,"nbformat_minor":2}
