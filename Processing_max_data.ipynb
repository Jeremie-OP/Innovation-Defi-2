{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["SVN approche\n","Ce book integre:\n","    la recolte d'informations sur les commentaires\n","    la preparation des données pour etre traités par liblinear\n","    la traitement de la sorti de liblinear"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from elasticsearch import Elasticsearch, helpers\n","import pickle\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from tqdm.gui import tqdm as tqdm_gui\n"]},{"cell_type":"markdown","metadata":{},"source":["AJout des functions d'import des data"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def xmlToDf(xmlFile):\n","    # Read XML file\n","    df = pd.read_xml(xmlFile)\n","    # replace comma to point in note column\n","    df[\"note\"] = df[\"note\"].apply(lambda x: x.replace(\",\", \".\"))\n","    df[\"commentaire\"] = df[\"commentaire\"].apply(checkIfWordInComment)\n","    # string to double conversion column\n","    df['note'] = df['note'].astype(float)\n","    return df\n","\n","def checkIfWordInComment(comment):\n","    if comment is None:\n","        return \"\"\n","    return comment\n","\n","def getNbUpperCases(comment):\n","    if comment is None:\n","        return 0\n","    return sum(1 for c in comment if c.isupper())\n","\n","def getSizeString(comment):\n","    if comment is None:\n","        return 0\n","    return int(len(comment))\n","\n","def getCountWords(comment):\n","    if comment is None:\n","        return 0\n","    return int(len(comment.split()))\n","\n","def getCountExclamation(comment):\n","    if comment is None:\n","        return 0\n","    return int(comment.count('!'))\n","\n","def getCountQuestion(comment):\n","    if comment is None:\n","        return 0\n","    return int(comment.count('?'))\n","\n","def getCountPoint(comment):\n","    if comment is None:\n","        return 0\n","    return int(comment.count('.'))\n","    \n","def getCountSmiley(comment):\n","    if comment is None:\n","        return 0\n","    return int(comment.count(':)'or ':(' or ':D' or 'xD'))\n","\n","def getCountPas(comment):\n","    if comment is None:\n","        return 0\n","    return int(comment.lower().count('pas'))\n","\n","def getCountGoodList(comment):\n","    if comment is None:\n","        return 0\n","    count = 0\n","    for word in good_list:\n","        count += comment.lower().count(word)\n","    return count\n","\n","def getCountBadList(comment):\n","    if comment is None:\n","        return 0\n","    count = 0\n","    for word in bad_list:\n","        count += comment.lower().count(word)\n","    return count\n","\n","def getClassRate(note):\n","    if note < 2:\n","        return 0\n","    elif note > 2:\n","        return 2\n","    else:\n","        return 1\n","\n","def loadGoodList():\n","    with open('../Projet/dict/good.txt', 'r') as f:\n","        return f.read().splitlines()\n","\n","def loadBadList():\n","    with open('../Projet/dict/bad.txt', 'r') as f:\n","        return f.read().splitlines()\n","\n","def loadAndGetBasicInfo(path):\n","    df_dev = xmlToDf(path)\n","    df_dev['class_rate'] = df_dev['note'].apply(getClassRate)    \n","    df_dev['size_char'] = df_dev['commentaire'].apply(getSizeString)\n","    print(\"Got size_char\")\n","    df_dev['uppercase'] = df_dev['commentaire'].apply(getNbUpperCases)\n","    print('Got uppercase')\n","    df_dev['ratio'] = df_dev['size_char']/df_dev['uppercase']\n","    print('Got ratio')\n","    df_dev['count_words'] = df_dev['commentaire'].apply(getCountWords)\n","    print('Got count_words')\n","    df_dev['count_exclamation'] = df_dev['commentaire'].apply(getCountExclamation)\n","    print('Got count_exclamation')\n","    df_dev['count_question'] = df_dev['commentaire'].apply(getCountQuestion)\n","    print('Got count_question')\n","    df_dev['count_point'] = df_dev['commentaire'].apply(getCountPoint)\n","    print('Got count_point')\n","    df_dev['count_smiley'] = df_dev['commentaire'].apply(getCountSmiley)\n","    print('Got count_smiley')\n","    df_dev['count_pas'] = df_dev['commentaire'].apply(getCountPas)\n","    print('Got count_pas')    \n","    tqdm.pandas(desc=\"Counting bad words\")\n","    df_dev['count_mal'] = df_dev['commentaire'].progress_apply(lambda x: getCountBadList(x))\n","    print('Got count_mal')    \n","    tqdm.pandas(desc=\"Counting good words\")\n","    df_dev['count_good'] = df_dev['commentaire'].progress_apply(lambda x: getCountGoodList(x))\n","    print('Got count_mal')\n","    return df_dev\n","\n","bad_list = loadBadList()\n","good_list = loadGoodList()"]},{"cell_type":"markdown","metadata":{},"source":["Import des data dev\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\wiakx\\OneDrive\\1Documents\\Cours\\M2\\INOV2\\Projet\\Projet.ipynb Cellule 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_dev \u001b[39m=\u001b[39m loadAndGetBasicInfo(\u001b[39m\"\u001b[39;49m\u001b[39mdata/dev.xml\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m (df_dev\u001b[39m.\u001b[39mkeys())\n","\u001b[1;32mc:\\Users\\wiakx\\OneDrive\\1Documents\\Cours\\M2\\INOV2\\Projet\\Projet.ipynb Cellule 5\u001b[0m in \u001b[0;36mloadAndGetBasicInfo\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloadAndGetBasicInfo\u001b[39m(path):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     df_dev \u001b[39m=\u001b[39m xmlToDf(path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     df_dev[\u001b[39m'\u001b[39m\u001b[39mclass_rate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_dev[\u001b[39m'\u001b[39m\u001b[39mnote\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(getClassRate)    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     df_dev[\u001b[39m'\u001b[39m\u001b[39msize_char\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_dev[\u001b[39m'\u001b[39m\u001b[39mcommentaire\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(getSizeString)\n","\u001b[1;32mc:\\Users\\wiakx\\OneDrive\\1Documents\\Cours\\M2\\INOV2\\Projet\\Projet.ipynb Cellule 5\u001b[0m in \u001b[0;36mxmlToDf\u001b[1;34m(xmlFile)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mxmlToDf\u001b[39m(xmlFile):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Read XML file\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_xml(xmlFile)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# replace comma to point in note column\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mnote\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mnote\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m))\n","File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\xml.py:938\u001b[0m, in \u001b[0;36mread_xml\u001b[1;34m(path_or_buffer, xpath, namespaces, elems_only, attrs_only, names, encoding, parser, stylesheet, compression, storage_options)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(\n\u001b[0;32m    739\u001b[0m     version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mpath_or_buffer\u001b[39m\u001b[39m\"\u001b[39m], stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m    740\u001b[0m )\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    757\u001b[0m     storage_options: StorageOptions \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    758\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m    759\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[39m    Read XML document into a ``DataFrame`` object.\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[39m    2  triangle      180    3.0\u001b[39;00m\n\u001b[0;32m    936\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 938\u001b[0m     \u001b[39mreturn\u001b[39;00m _parse(\n\u001b[0;32m    939\u001b[0m         path_or_buffer\u001b[39m=\u001b[39;49mpath_or_buffer,\n\u001b[0;32m    940\u001b[0m         xpath\u001b[39m=\u001b[39;49mxpath,\n\u001b[0;32m    941\u001b[0m         namespaces\u001b[39m=\u001b[39;49mnamespaces,\n\u001b[0;32m    942\u001b[0m         elems_only\u001b[39m=\u001b[39;49melems_only,\n\u001b[0;32m    943\u001b[0m         attrs_only\u001b[39m=\u001b[39;49mattrs_only,\n\u001b[0;32m    944\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    945\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    946\u001b[0m         parser\u001b[39m=\u001b[39;49mparser,\n\u001b[0;32m    947\u001b[0m         stylesheet\u001b[39m=\u001b[39;49mstylesheet,\n\u001b[0;32m    948\u001b[0m         compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    949\u001b[0m         storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    950\u001b[0m     )\n","File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\xml.py:733\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(path_or_buffer, xpath, namespaces, elems_only, attrs_only, names, encoding, parser, stylesheet, compression, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValues for parser can only be lxml or etree.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 733\u001b[0m data_dicts \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mparse_data()\n\u001b[0;32m    735\u001b[0m \u001b[39mreturn\u001b[39;00m _data_to_frame(data\u001b[39m=\u001b[39mdata_dicts, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\xml.py:389\u001b[0m, in \u001b[0;36m_LxmlFrameParser.parse_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39mParse xml data.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mand parse original or transformed XML and return specific nodes.\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlxml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39metree\u001b[39;00m \u001b[39mimport\u001b[39;00m XML\n\u001b[1;32m--> 389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxml_doc \u001b[39m=\u001b[39m XML(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_doc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath_or_buffer))\n\u001b[0;32m    391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstylesheet \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxsl_doc \u001b[39m=\u001b[39m XML(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_doc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstylesheet))\n","File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\xml.py:560\u001b[0m, in \u001b[0;36m_LxmlFrameParser._parse_doc\u001b[1;34m(self, raw_doc)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    558\u001b[0m         doc \u001b[39m=\u001b[39m parse(xml_data, parser\u001b[39m=\u001b[39mcurr_parser)\n\u001b[1;32m--> 560\u001b[0m \u001b[39mreturn\u001b[39;00m tostring(doc)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["df_dev = loadAndGetBasicInfo(\"data/dev.xml\")\n","print (df_dev.keys())"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Got size_char\n","Got uppercase\n","Got ratio\n","Got count_words\n","Got count_exclamation\n","Got count_question\n","Got count_point\n","Got count_smiley\n","Got count_pas\n"]},{"name":"stderr","output_type":"stream","text":["Counting bad words: 100%|██████████| 665962/665962 [00:16<00:00, 40415.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Got count_mal\n"]},{"name":"stderr","output_type":"stream","text":["Counting good words: 100%|██████████| 665962/665962 [00:16<00:00, 40445.88it/s]"]},{"name":"stdout","output_type":"stream","text":["Got count_mal\n","Index(['movie', 'review_id', 'name', 'user_id', 'note', 'commentaire',\n","       'class_rate', 'size_char', 'uppercase', 'ratio', 'count_words',\n","       'count_exclamation', 'count_question', 'count_point', 'count_smiley',\n","       'count_pas', 'count_mal', 'count_good'],\n","      dtype='object')\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["df_train = loadAndGetBasicInfo(\"data/train.xml\")\n","print (df_train.keys())"]},{"cell_type":"markdown","metadata":{},"source":["Import des data via deserialize"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def deserializeDf(path):\n","    with open(path, 'rb') as f:\n","        return pickle.load(f)\n","    \n","# df_dev = deserializeDf('data/df_dev.pkl')\n","# df_idf = deserializeDf('data/df_idf.pkl')\n","df_train = deserializeDf('data/df_train.pkl')\n","#df_idf_train = deserializeDf('data/df_idf_train.pkl')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["stop_words = [\" \", \"l'\", \"l’\", \"la\", \"le\", \"les\", \"d’\", \"d'\", \"de\", \"du\", \"des\", \"une\", \"un\",\n","                \"ce\", \"ces\", \"je\", \"moi\", \"mon\", \"me\", \"mes\", \"tu\", \"toi\", \"ton\", \"te\", \"tes\", \n","                \"il\", \"lui\", \"son\", \"se\", \"ses\", \"nous\", \"notre\", \"nos\", \"vous\", \"votre\", \"vos\",\n","                \"ils\", \"leur\", \"leurs\", \"n'\", \"ne\", \"tout\", \"être\", \"avoir\", \"deja\", \"déjà\",\n","                \"ou\" ,\"où\", \"qu’\", \"qu'\", \"que\", \"qui\", \"quelle\", \"quel\", \"quelles\", \"quels\", \n","                \".\", \",\", \"...\", \"sur\", \"telle\", \"tel\", \"telles\", \"tels\", \"laquelle\", \"lequel\",\n","                \"laquelles\", \"lequels\", \"simplement\", \"comment\", \"quoi\", \"dont\", \"donc\", \"tant\",\n","                \"jamais\", \"rarement\", \"parfois\", \"souvent\", \"toujours\", \"avec\", \"pour\", \"ici\",\n","                \":\", \"(\", \")\", \"[\", \"]\", \"\\\"\", \"y\", \"et\", \"par\", \"fois\", \"peu\", \"on\", \"cela\",\n","                \"mais\", \"dans\", \"en\", \"à\", \"au\", \"même\", \"là\", \"-\", \"si\", \"comme\", \"aussi\",\n","                \"car\", \"parce\", \"quand\"]"]},{"cell_type":"markdown","metadata":{},"source":["Separation de la data en 3 classes (0.5-1.5 / 2 / 2.5-5)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["df_low = df_dev.loc[df_dev['note'] < 2]\n","df_mid = df_dev.loc[(df_dev['note'] == 2)]\n","df_high = df_dev.loc[df_dev['note'] > 2]\n","df_dev['class_rate'] = df_dev['note'].apply(getClassRate) "]},{"cell_type":"markdown","metadata":{},"source":["Des graphs"]},{"cell_type":"markdown","metadata":{},"source":["Normalisation + stemming"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  (0, 19757)\t0.18687771683382542\n","  (0, 48901)\t0.19070082913350986\n","  (0, 83973)\t0.03842537283589266\n","  (0, 19314)\t0.12576934176003657\n","  (0, 88498)\t0.10989720432141713\n","  (0, 32824)\t0.07478572548905398\n","  (0, 61769)\t0.07723367066740257\n","  (0, 71022)\t0.135551918556739\n","  (0, 79063)\t0.09634334825587375\n","  (0, 68853)\t0.031621853764034064\n","  (0, 90127)\t0.05416056160163361\n","  (0, 58850)\t0.08835171409701363\n","  (0, 71831)\t0.11924719046445877\n","  (0, 92322)\t0.02717659835194249\n","  (0, 88448)\t0.08192120620741246\n","  (0, 92287)\t0.022038799711691436\n","  (0, 46918)\t0.127714871290098\n","  (0, 76354)\t0.1209863061717123\n","  (0, 66526)\t0.05101464405073643\n","  (0, 63071)\t0.030983157942342035\n","  (0, 47014)\t0.18687771683382542\n","  (0, 97566)\t0.1209132443451202\n","  (0, 64725)\t0.09564533630715794\n","  (0, 33424)\t0.15676038569827785\n","  (0, 21448)\t0.09448442761009675\n","  :\t:\n","  (100399, 72074)\t0.061359460769206585\n","  (100399, 2636)\t0.07123186957143746\n","  (100399, 8753)\t0.05516281640257527\n","  (100399, 68853)\t0.10107447111897357\n","  (100399, 92322)\t0.04343294238428818\n","  (100399, 66526)\t0.08153029555489726\n","  (100399, 63071)\t0.04951648828032435\n","  (100399, 53031)\t0.04240743594617035\n","  (100399, 66481)\t0.11724288290557584\n","  (100399, 87946)\t0.10708871123319345\n","  (100399, 37467)\t0.035875985290216475\n","  (100399, 36339)\t0.06701737438266496\n","  (100399, 86373)\t0.1244028304261439\n","  (100399, 34752)\t0.10249310200257773\n","  (100399, 52026)\t0.11665400531155283\n","  (100399, 72309)\t0.045935743025608454\n","  (100399, 37445)\t0.12827984469748246\n","  (100399, 24575)\t0.04791243190205182\n","  (100399, 23781)\t0.13359807998561313\n","  (100399, 32443)\t0.045728160689779966\n","  (100399, 72219)\t0.09685035178573383\n","  (100399, 52778)\t0.14691417718040642\n","  (100399, 65134)\t0.04457643225115032\n","  (100399, 34622)\t0.22490861817593397\n","  (100399, 15624)\t0.04675394240755744\n"]}],"source":["def TfIdf(df, column):\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    vectorizer = TfidfVectorizer()\n","    X = vectorizer.fit_transform(df[column])\n","    return X\n","\n","x = TfIdf(df_dev, 'commentaire')\n","print(x)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idf_weights</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>de</th>\n","      <td>1.219785</td>\n","    </tr>\n","    <tr>\n","      <th>et</th>\n","      <td>1.247718</td>\n","    </tr>\n","    <tr>\n","      <th>un</th>\n","      <td>1.286338</td>\n","    </tr>\n","    <tr>\n","      <th>film</th>\n","      <td>1.310228</td>\n","    </tr>\n","    <tr>\n","      <th>le</th>\n","      <td>1.341365</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>inerent</th>\n","      <td>11.823780</td>\n","    </tr>\n","    <tr>\n","      <th>inerprété</th>\n","      <td>11.823780</td>\n","    </tr>\n","    <tr>\n","      <th>inerprétée</th>\n","      <td>11.823780</td>\n","    </tr>\n","    <tr>\n","      <th>indétronâble</th>\n","      <td>11.823780</td>\n","    </tr>\n","    <tr>\n","      <th>黄海</th>\n","      <td>11.823780</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>99180 rows × 1 columns</p>\n","</div>"],"text/plain":["              idf_weights\n","de               1.219785\n","et               1.247718\n","un               1.286338\n","film             1.310228\n","le               1.341365\n","...                   ...\n","inerent         11.823780\n","inerprété       11.823780\n","inerprétée      11.823780\n","indétronâble    11.823780\n","黄海              11.823780\n","\n","[99180 rows x 1 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#instantiate CountVectorizer() \n","cv=CountVectorizer() \n","# this steps generates word counts for the words in your docs \n","\n","word_count_vector=cv.fit_transform(df_dev['commentaire'])\n","tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n","tfidf_transformer.fit(word_count_vector)\n","# print idf values \n","df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(),columns=[\"idf_weights\"]) \n","# sort ascending \n","df_idf.sort_values(by=['idf_weights'])"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idf_weights</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>de</th>\n","      <td>1.217917</td>\n","    </tr>\n","    <tr>\n","      <th>et</th>\n","      <td>1.244742</td>\n","    </tr>\n","    <tr>\n","      <th>un</th>\n","      <td>1.279303</td>\n","    </tr>\n","    <tr>\n","      <th>film</th>\n","      <td>1.308810</td>\n","    </tr>\n","    <tr>\n","      <th>le</th>\n","      <td>1.338168</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>ilustrées</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>ilétéoùheinoussama</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>im6</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>imersif</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>𝙑𝙊𝙎</th>\n","      <td>13.715842</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>238779 rows × 1 columns</p>\n","</div>"],"text/plain":["                    idf_weights\n","de                     1.217917\n","et                     1.244742\n","un                     1.279303\n","film                   1.308810\n","le                     1.338168\n","...                         ...\n","ilustrées             13.715842\n","ilétéoùheinoussama    13.715842\n","im6                   13.715842\n","imersif               13.715842\n","𝙑𝙊𝙎                   13.715842\n","\n","[238779 rows x 1 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#instantiate CountVectorizer() \n","cv=CountVectorizer() \n","# this steps generates word counts for the words in your docs \n","\n","word_count_vector=cv.fit_transform(df_train['commentaire'])\n","tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n","tfidf_transformer.fit(word_count_vector)\n","# print idf values \n","df_idf_train = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(),columns=[\"idf_weights\"]) \n","# sort ascending \n","df_idf_train.sort_values(by=['idf_weights'])"]},{"cell_type":"markdown","metadata":{},"source":["trie des stopwords + Stemming des mots"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\wiakx\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["['on', 'transforme', 'le', 'liste', 'de', 'sentiment', 'en', 'tenseur']\n","['transforme', 'liste', 'sentiment', 'tenseur']\n"]}],"source":["import spacy\n","import nltk\n","import multiprocessing as mp\n","\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","stemmer = SnowballStemmer(language='french')\n","nltk.download('stopwords')\n","stopWords = set(stopwords.words('french'))\n","nlp = spacy.load(\"fr_core_news_sm\")\n","\n","def getStemWord(comment):\n","    if comment is None:\n","        return \"\"\n","    doc = nlp(comment)\n","    tokens = [X.text for X in doc]\n","    clean_words = []\n","    for token in tokens:\n","        if token not in stopWords:\n","            clean_words.append(token)\n","    return [stemmer.stem(token) for token in clean_words]\n","\n","def getProperName(comment):\n","    if comment is None:\n","        return \"\"\n","    doc = nlp(comment)\n","    return [(X.text, X.label_) for X in doc.ents]\n","\n","\n","tqdm.pandas(desc=\"Stemming words\")\n","df_train['stem_word'] = df_train['commentaire'].progress_apply(lambda x: getStemWord(x))\n","df_train['proper_name'] = df_train['commentaire'].progress_apply(lambda x: getProperName(x))\n","# df_dev['stem_word'] = df_dev['commentaire'].progress_apply(lambda x: getStemWord(x))\n","# df_dev['proper_name'] = df_dev['commentaire'].progress_apply(lambda x: getProperName(x))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\wiakx\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["on commence\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100400/100400 [1:35:48<00:00, 17.47it/s] \n"]}],"source":["import spacy\n","import nltk\n","import multiprocessing as mp\n","import concurrent.futures\n","\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","# stopWords = set(stopwords.words('french'))\n","# nlp = spacy.load(\"fr_core_news_sm\") # more efficient but less accurate\n","nlp = spacy.load(\"fr_dep_news_trf\") # less efficient but more accurate\n","\n","def getLemWord(comment):\n","    if comment is None:\n","        return \"\"\n","    doc = nlp(comment)\n","    tokens = [X.lemma_ for X in doc]\n","    clean_words = []\n","    for token in tokens:\n","        if token not in stop_words:\n","            tokens.remove(token)\n","    return tokens\n","\n","\n","\n","tqdm.pandas(desc=\"Lemma words\")\n","df_train['lemma_word'] = df_train['commentaire'].progress_apply(lambda x: getLemWord(x))\n","# df_dev['lemma_word'] = df_dev['commentaire'].progress_apply(lambda x: getLemWord(x))\n","# df_dev['proper_name'] = df_dev['commentaire'].progress_apply(lambda x: getProperName(x))\n","\n","comments = df_dev['commentaire'].tolist()\n","print('on commence')\n","with concurrent.futures.ThreadPoolExecutor(max_workers=mp.cpu_count()) as executor:\n","    results = list(tqdm(executor.map(getLemWord, comments), total=len(comments)))\n","\n","\n","df_dev['lemma_word'] = results"]},{"cell_type":"markdown","metadata":{},"source":["Redivision des données en classe selon les notes"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------------\n","Low\n","\n",", 65003\n",". 53775\n","film 23133\n","! 15705\n","d' 15523\n","l' 15518\n","a 13247\n","\" 11370\n","c' 9739\n","... 9619\n","-----------------\n","Mid\n","\n",", 42438\n",". 34898\n","film 13736\n","l' 10629\n","d' 9512\n","a 7670\n","\" 7034\n","plus 5683\n","tout 5548\n","! 5457\n","-----------------\n","High\n","\n",", 375462\n",". 320961\n","film 136172\n","l' 100379\n","d' 88764\n","! 67648\n","a 63611\n","\" 55377\n","bien 50641\n","tout 50013\n","-----------------\n","End\n"]}],"source":["df_low = df_dev.loc[df_dev['note'] < 2]\n","df_mid = df_dev.loc[(df_dev['note'] == 2)]\n","df_high = df_dev.loc[df_dev['note'] > 2]\n","\n","\n","def dictFreqEachWords(df):\n","    dictFreq = {}\n","    for index, row in df.iterrows():\n","        for word in row['stem_word']:\n","            if word in dictFreq:\n","                dictFreq[word] += 1\n","            else:\n","                dictFreq[word] = 1\n","    return dictFreq\n","\n","def OrderDictByValue(dictFreq):\n","    return {k: v for k, v in sorted(dictFreq.items(), key=lambda item: item[1], reverse=True)}\n","\n","def PrintXFirstDict(dictFreq, x):\n","    for key, value in dictFreq.items():\n","        if x > 0:\n","            print(key, value)\n","            x -= 1\n","        else:\n","            break\n","    return\n","\n","dict_low = dictFreqEachWords(df_low)\n","dict_mid = dictFreqEachWords(df_mid)\n","dict_high = dictFreqEachWords(df_high)\n","dict_low = OrderDictByValue(dict_low)\n","dict_mid = OrderDictByValue(dict_mid)\n","dict_high = OrderDictByValue(dict_high)\n","\n","print('-----------------')\n","print('Low\\n')\n","PrintXFirstDict(dict_low, 10)\n","print('-----------------')\n","print('Mid\\n')\n","PrintXFirstDict(dict_mid, 10)\n","print('-----------------')\n","print('High\\n')\n","PrintXFirstDict(dict_high, 10)\n","\n","print('-----------------')\n","print('End')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'dict_high' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m             dict_low\u001b[39m.\u001b[39mpop(listWord[i])\n\u001b[0;32m      7\u001b[0m             dict_mid\u001b[39m.\u001b[39mpop(listWord[i])\n\u001b[1;32m----> 9\u001b[0m removeSimilarInDicts()\n","Cell \u001b[1;32mIn[11], line 2\u001b[0m, in \u001b[0;36mremoveSimilarInDicts\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremoveSimilarInDicts\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     listWord \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(dict_high\u001b[39m.\u001b[39mkeys())[:\u001b[39m30\u001b[39m]\n\u001b[0;32m      3\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m30\u001b[39m):\n\u001b[0;32m      4\u001b[0m         \u001b[39mif\u001b[39;00m listWord[i] \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(dict_mid\u001b[39m.\u001b[39mkeys())[:\u001b[39m30\u001b[39m] \u001b[39mor\u001b[39;00m listWord[i] \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(dict_low\u001b[39m.\u001b[39mkeys())[:\u001b[39m30\u001b[39m]:\n","\u001b[1;31mNameError\u001b[0m: name 'dict_high' is not defined"]}],"source":["def removeSimilarInDicts():\n","    listWord = list(dict_high.keys())[:30]\n","    for i in range(0, 30):\n","        if listWord[i] in list(dict_mid.keys())[:30] or listWord[i] in list(dict_low.keys())[:30]:\n","            dict_high.pop(listWord[i])\n","            dict_low.pop(listWord[i])\n","            dict_mid.pop(listWord[i])\n","            \n","removeSimilarInDicts()"]},{"cell_type":"markdown","metadata":{},"source":["Serialize df into file (pour save)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def serializeDf(df, path):\n","    with open(path, 'wb') as f:\n","        pickle.dump(df, f)\n","        \n","serializeDf(df_dev, 'data/df_dev.pkl')\n","# serializeDf(df_idf, 'data/df_idf.pkl')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def serializeDf(df, path):\n","    with open(path, 'wb') as f:\n","        pickle.dump(df, f)\n","        \n","serializeDf(df_train, 'data/df_train.pkl')\n","#serializeDf(df_idf_train, 'data/df_idf_train.pkl')"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'df_dev' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\wiakx\\OneDrive\\1Documents\\Cours\\M2\\INOV2\\Projet\\Projet.ipynb Cellule 32\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             row[\u001b[39m'\u001b[39m\u001b[39mstem_word\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mremove(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m removeCommaInDF()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(df_dev[\u001b[39m'\u001b[39m\u001b[39mstem_word\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m10\u001b[39m])\n","\u001b[1;31mNameError\u001b[0m: name 'df_dev' is not defined"]}],"source":["def removeCommaInDF():\n","    for index, row in df_train.iterrows():\n","        if row['lemma_word'].count(',') > 0:\n","            row['lemma_word'].remove(',')\n","\n","removeCommaInDF()\n","print(df_train['lemma_word'][:10])"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def listToString(s):  \n","    str1 = \" \" \n","    return (str1.join(s))\n","\n","# df_train['stem_word_string'] = df_train['stem_word'].apply(lambda x: listToString(x))\n","df_train['lemma_word_string'] = df_train['lemma_word'].apply(lambda x: listToString(x))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idf_stem_weights</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>film</th>\n","      <td>1.276222</td>\n","    </tr>\n","    <tr>\n","      <th>bien</th>\n","      <td>1.971428</td>\n","    </tr>\n","    <tr>\n","      <th>tout</th>\n","      <td>2.002309</td>\n","    </tr>\n","    <tr>\n","      <th>tres</th>\n","      <td>2.011759</td>\n","    </tr>\n","    <tr>\n","      <th>bon</th>\n","      <td>2.025595</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>hypeee</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>hype</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>hypcondriaqu</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>hypercar</th>\n","      <td>13.715842</td>\n","    </tr>\n","    <tr>\n","      <th>𝙑𝙊𝙎</th>\n","      <td>13.715842</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>148062 rows × 1 columns</p>\n","</div>"],"text/plain":["              idf_stem_weights\n","film                  1.276222\n","bien                  1.971428\n","tout                  2.002309\n","tres                  2.011759\n","bon                   2.025595\n","...                        ...\n","hypeee               13.715842\n","hype                 13.715842\n","hypcondriaqu         13.715842\n","hypercar             13.715842\n","𝙑𝙊𝙎                  13.715842\n","\n","[148062 rows x 1 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#instantiate CountVectorizer() \n","cv=CountVectorizer() \n","# this steps generates word counts for the words in your docs \n","\n","word_count_vector=cv.fit_transform(df_train['stem_word_string'])\n","tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n","tfidf_transformer.fit(word_count_vector)\n","# print idf values \n","df_idf_train = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(),columns=[\"idf_stem_weights\"]) \n","# sort ascending \n","df_idf_train.sort_values(by=['idf_stem_weights'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def serializeDf(df, path):\n","    with open(path, 'wb') as f:\n","        pickle.dump(df, f)\n","        \n","serializeDf(df_train, 'data/df_train.pkl')\n","serializeDf(df_idf_train, 'data/df_idf_train.pkl')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\sdk\\python10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Deletings useless words: 100%|██████████| 665962/665962 [22:24<00:00, 495.41it/s] \n"]}],"source":["from multiprocessing import Pool\n","from tqdm.contrib.concurrent import process_map\n","\n","def stemWordLow(stem_word):\n","    if stem_word is None:\n","        return []\n","    clean_words = []\n","    for token in stem_word:\n","        if token in df_idf_train.index and df_idf_train.loc[token]['idf_stem_weights'] < 10:\n","            clean_words.append(token)\n","    return clean_words\n","\n","def stemWordSuperLow(stem_word):\n","    if stem_word is None:\n","        return []\n","    clean_words = []\n","    for token in stem_word:\n","        if token in df_idf_train.index and df_idf_train.loc[token]['idf_stem_weights'] < 9:\n","            clean_words.append(token)\n","    return clean_words\n","\n","def removeUselessWords():\n","    df_train['stem_word_low'] = df_train['stem_word'].apply(lambda x: stemWordLow(x))\n","\n","def removeSuperUselessWords():\n","    tqdm.pandas(desc=\"Deletings useless words\")\n","    df_train['stem_word_super_low'] = df_train['stem_word'].progress_apply(lambda x: stemWordSuperLow(x))\n","\n","removeUselessWords()\n","# removeSuperUselessWords()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unexpected exception formatting exception. Falling back to standard exception\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"C:\\Users\\wiakx\\AppData\\Local\\Temp\\ipykernel_16692\\257940883.py\", line 9, in <module>\n","    tmp = getUniqueToken()\n","  File \"C:\\Users\\wiakx\\AppData\\Local\\Temp\\ipykernel_16692\\257940883.py\", line -1, in getUniqueToken\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2052, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n","    frames.append(self.format_record(r))\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n","    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 734, in lines\n","    pieces = self.included_pieces\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n","    pos = scope_pieces.index(self.executing_piece)\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n","    return only(\n","  File \"C:\\Users\\wiakx\\AppData\\Roaming\\Python\\Python310\\site-packages\\executing\\executing.py\", line 190, in only\n","    raise NotOneValueFound('Expected one value, found 0')\n","executing.executing.NotOneValueFound: Expected one value, found 0\n"]}],"source":["#get all unique token in df_train['stem_word_super_low']\n","def getUniqueToken():\n","    listToken = []\n","    for index, row in df_train.iterrows():\n","        for token in row['stem_word_super_low']:\n","            if token not in listToken:\n","                listToken.append(token)\n","    return listToken\n","tmp = getUniqueToken()\n","print(len(tmp))"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["def create_dict_of_words():\n","    word_to_id = {}\n","    index = 0\n","    # for comment in df_dev['stem_word_low']:\n","    for comment in df_train['stem_word_super_low']:\n","        for token in comment:\n","            if token not in word_to_id:\n","                word_to_id[token] = index\n","                index += 1\n","    return word_to_id\n","\n","def create_dict_of_bigrams():\n","    bigram_to_id = {}\n","    multiple_bigram_to_id = {}\n","    index = 0\n","    multiple_index = 0\n","    # for comment in df_dev['stem_word_low']:\n","    for comment in df_train['stem_word_super_low']:\n","        for i in range(len(comment) - 1):\n","            bigram = (comment[i], comment[i+1])\n","            if bigram not in bigram_to_id:\n","                bigram_to_id[bigram] = index\n","                index += 1\n","            else:\n","                if bigram not in multiple_bigram_to_id:\n","                    multiple_bigram_to_id[bigram] = multiple_index\n","                    multiple_index += 1\n","    return multiple_bigram_to_id\n","\n","word_to_id = create_dict_of_words()\n","bigram_to_id = create_dict_of_bigrams()\n"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["def comment_to_vec(comment):\n","    vector = [0] * len(word_to_id)\n","    for token in comment:\n","        vector[word_to_id[token]] += 1\n","    return vector\n","\n","def vector_to_svm(vector):\n","    svm = ''\n","    for i in range(len(vector)):\n","        if vector[i] > 0:\n","            svm += str(i+1) + ':' + str(vector[i]) + ' '\n","    return svm\n","\n","def comment_to_svm(comment, note):\n","    dict_comment = {}\n","    for token in comment:\n","        dict_comment[token] = dict_comment.get(token, 0) + 1\n","    dict_bi_comment = {}\n","    for i in range(len(comment) - 1):\n","        bigram = (comment[i], comment[i+1])\n","        dict_bi_comment[bigram] = dict_bi_comment.get(bigram, 0) + 1\n","    svm = '' + str(note*2) + ' '\n","    #sort by key with word_to_id\n","    # dict_comment = {k: v for k, v in sorted(dict_comment.items(), key=lambda item: word_to_id[item[0]])}\n","    # for token in dict_comment:\n","    #     svm += str(word_to_id[token]+1) + ':' + str(dict_comment[token]) + ' '\n","    \n","    tmp = {}\n","    #remove bigram from dict_bi_comment if it is in bigram_to_id\n","    for token in dict_bi_comment:\n","        if bigram_to_id.get(token) is not None:\n","            tmp[token] = dict_bi_comment[token]\n","    dict_bi_comment = tmp\n","    #sort by key with bigram_to_id\n","    dict_bi_comment = {k: v for k, v in sorted(dict_bi_comment.items(), key=lambda item: bigram_to_id[item[0]])}\n","    for token in dict_bi_comment:\n","        if bigram_to_id.get(token) is not None:\n","            svm += str(bigram_to_id[token]+1) + ':' + str(dict_bi_comment[token]) + ' '\n","            # svm += str(bigram_to_id[token]+1+len(word_to_id)) + ':' + str(dict_bi_comment[token]) + ' '\n","    return svm\n","    \n","\n","def comment_to_svm_class(comment, note):\n","    dict_comment = {}\n","    for token in comment:\n","        if (word_to_id.get(token) is not None):\n","            dict_comment[token] = dict_comment.get(token, 0) + 1\n","    if note < 2:\n","        svm = '' + str(1) + ' '\n","    elif note == 2:\n","        svm = '' + str(2) + ' '\n","    else:\n","        svm = '' + str(3) + ' '\n","    #sort by key with word_to_id\n","    dict_comment = {k: v for k, v in sorted(dict_comment.items(), key=lambda item: word_to_id[item[0]])}\n","    for token in dict_comment:\n","        svm += str(word_to_id[token]+1) + ':' + str(dict_comment[token]) + ' '\n","    return svm"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["comment_low_train = df_train['stem_word_super_low'].values.tolist()\n","note_train = df_train['note'].values.tolist()\n","del(df_train)\n","#del(df_idf_train)"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 665962/665962 [01:43<00:00, 6440.59it/s]\n"]}],"source":["svms = []\n","for index, comment in enumerate(tqdm(comment_low_train)):\n","    svm = comment_to_svm(comment, note_train[index])\n","    svms.append(svm)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 665962/665962 [00:36<00:00, 18009.08it/s]\n"]}],"source":["svms = []\n","for index, comment in enumerate(tqdm(comment_low_train)):\n","    svm = comment_to_svm_class(comment, note_train[index])\n","    svms.append(svm)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["# save svms in svm file\n","with open('data/SVM_TRAIN_10_only_bi.svm', 'w') as f:\n","    for svm in svms:\n","        f.write(svm + '\\n')"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 100400/100400 [01:27<00:00, 1142.96it/s]\n"]}],"source":["svmsC = []\n","for i ,vector in enumerate(tqdm(vectors)):\n","    if(df_dev['note'][i] < 2):\n","        svm = str(1) + ' ' + vector_to_svm(vector)\n","    elif(df_dev['note'][i] == 2):\n","        svm = str(2) + ' ' + vector_to_svm(vector)\n","    else:\n","        svm = str(3) + ' ' + vector_to_svm(vector)\n","    svmsC.append(svm)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# save svms in svm file\n","with open('data/svmsT.svm', 'w') as f:\n","    for svm in svms:\n","        f.write(svm + '\\n')\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["    movie        review_id  name                   user_id  note  \\\n","0  229864  review_58145147  AM11  Z20051105131449940367618   4.0   \n","\n","                                         commentaire  class_rate  size_char  \\\n","0  \"Ce n'est pas ce qui se trouve en face de vous...           2       1186   \n","\n","   uppercase  ratio  ...  count_question  count_point  count_smiley  \\\n","0         10  118.6  ...               0            8             0   \n","\n","   count_pas  count_mal  count_good  \\\n","0          4          0           3   \n","\n","                                           stem_word  \\\n","0  [\", ce, n', trouv, fac, c', tient, côt, \", ., ...   \n","\n","                proper_name  \\\n","0  [(Joseph Kosinski, PER)]   \n","\n","                                    stem_word_string  \\\n","0  \" ce n' trouv fac c' tient côt \" . cet phras t...   \n","\n","                                       stem_word_low  \n","0  [ce, trouv, fac, tient, côt, cet, phras, tagli...  \n","\n","[1 rows x 22 columns]\n"]}],"source":["print(df_train.head(1))"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Pour les tests</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_test = deserializeDf('data/df_test.pkl')"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\wiakx\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","100%|██████████| 85847/85847 [00:08<00:00, 10005.81it/s]\n"]}],"source":["import spacy\n","import nltk\n","import multiprocessing as mp\n","\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","stemmer = SnowballStemmer(language='french')\n","nltk.download('stopwords')\n","stopWords = set(stopwords.words('french'))\n","nlp = spacy.load(\"fr_core_news_sm\")\n","\n","def getStemWord(comment):\n","    if comment is None:\n","        return \"\"\n","    doc = nlp(comment)\n","    tokens = [X.text for X in doc]\n","    clean_words = []\n","    for token in tokens:\n","        if token not in stopWords:\n","            clean_words.append(token)\n","    return [stemmer.stem(token) for token in clean_words]\n","\n","def xmlToDfTest(xmlFile):\n","    # Read XML file\n","    df = pd.read_xml(xmlFile)\n","    df[\"commentaire\"] = df[\"commentaire\"].apply(checkIfWordInComment)\n","    return df\n","\n","def checkIfWordInComment(comment):\n","    if comment is None:\n","        return \"\"\n","    return comment\n","\n","def stemWordLow(stem_word):\n","    if stem_word is None:\n","        return []\n","    clean_words = []\n","    for token in stem_word:\n","        if token in df_idf_train.index and df_idf_train.loc[token]['idf_stem_weights'] < 9:\n","            clean_words.append(token)\n","    return clean_words\n","\n","def comment_to_svm_test(comment):\n","    dict_comment = {}\n","    for token in comment:\n","        if (word_to_id.get(token) is not None):\n","            dict_comment[token] = dict_comment.get(token, 0) + 1\n","    dict_bi_comment = {}\n","    for i in range(len(comment)-1):\n","        token = comment[i] + ' ' + comment[i+1]\n","        if (bigram_to_id.get(token) is not None):\n","            dict_bi_comment[token] = dict_bi_comment.get(token, 0) + 1\n","    svm = '1 '\n","    #sort by key with word_to_id\n","    dict_comment = {k: v for k, v in sorted(dict_comment.items(), key=lambda item: word_to_id[item[0]])}\n","    # for token in dict_comment:\n","    #     svm += str(word_to_id[token]+1) + ':' + str(dict_comment[token]) + ' '\n","    \n","    tmp = {}\n","    #remove bigram from dict_bi_comment if it is not in bigram_to_id\n","    for token in dict_bi_comment:\n","        if bigram_to_id.get(token) is not None:\n","            tmp[token] = dict_bi_comment[token]\n","    dict_bi_comment = tmp\n","\n","    dict_bi_comment = {k: v for k, v in sorted(dict_bi_comment.items(), key=lambda item: bigram_to_id[item[0]])}\n","    for token in dict_bi_comment:\n","        svm += str(bigram_to_id[token]+1) + ':' + str(dict_bi_comment[token]) + ' '\n","        # svm += str(bigram_to_id[token]+1+len(word_to_id)) + ':' + str(dict_bi_comment[token]) + ' '\n","    return svm\n","\n","#df_test = xmlToDfTest('data/test.xml')\n","#tqdm.pandas(desc=\"Stemming\")\n","#df_test['stem_word'] = df_test['commentaire'].progress_apply(lambda x: getStemWord(x))\n","\n","comment_test = df_test['stem_word'].values.tolist()\n","\n","svm_test = []\n","for index, comment in enumerate(tqdm(comment_test)):\n","    svm = comment_to_svm_test(comment)\n","    svm_test.append(svm)\n","\n","\n","# save svms in svm file\n","with open('data/svm_test_only_bi.svm', 'w') as f:\n","    for svm in svm_test:\n","        f.write(svm + '\\n')\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["serializeDf(df_test, 'data/df_test.pkl')"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["result = []\n","with open('data/output10_low_bi_v2.txt', 'r') as f:\n","    result = f.readlines()\n","r = []\n","for i in range(len(result)):\n","    tmp = df_test['review_id'][i] + ' ' + str(float(result[i])/2)\n","    tmp = tmp.replace('\\n', '').replace('.', ',')\n","    r.append(tmp)\n","\n","with open('data/result10_low_bi_v2.txt', 'w') as f:\n","    for i in range(len(r)):\n","        f.write(r[i] + '\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["result = []\n","with open('data/output10.txt', 'r') as f:\n","    result = f.readlines()\n","r = []\n","for i in range(len(result)):\n","    tmp = df_test['review_id'][i] + ' ' + str(float(result[i])/2)\n","    tmp = tmp.replace('\\n', '').replace('.', ',')\n","    r.append(tmp)\n","\n","with open('data/result10.txt', 'w') as f:\n","    for i in range(len(r)):\n","        f.write(r[i] + '\\n')\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["    movie        review_id              name                   user_id  \\\n","0  253727  review_59354742  gimliamideselfes  Z20040827093310567684711   \n","1  253727  review_62570109            Yetcha  Z20030318104639813779116   \n","2  253727  review_58180650          TTNOUGAT  Z20071021153249553451578   \n","3  253727  review_57696986       titicaca120  Z20060317115155370578365   \n","4  253727  review_57736972           velocio  Z20040630141522227308769   \n","\n","                                         commentaire  \n","0  12 jours, un film que j'ai raté à sa sortie et...  \n","1  Ces lueurs dans ces yeux... Soit ils sont tous...  \n","2  Il s’agit d’un documentaire sans la moindre ac...  \n","3  un documentaire magnifique dans cette unité ly...  \n","4  Un documentaire très intéressant, quand bien m...  \n"]}],"source":["df_test = xmlToDfTest('data/test.xml')\n","print(df_test.head())"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"54db34dbb873d0124069a1b7e3692f2fcb3af91d00ed8e76b38ddecc02ef7a27"}}},"nbformat":4,"nbformat_minor":2}
