{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import seaborn\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW, AutoTokenizer\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import AutoModelForSequenceClassification, CamembertForMaskedLM, AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmlToDf(xmlFile):\n",
    "    # Read XML file\n",
    "    df = pd.read_xml(xmlFile)\n",
    "    # replace comma to point in note column\n",
    "    df[\"note\"] = df[\"note\"].apply(lambda x: x.replace(\",\", \".\"))\n",
    "    #replace None to empty string in commentaire column\n",
    "    df[\"commentaire\"] = df[\"commentaire\"].apply(checkIfWordInComment)\n",
    "    # string to double conversion column\n",
    "    df['note'] = df['note'].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def checkIfWordInComment(comment):\n",
    "    if comment is None:\n",
    "        return \"\"\n",
    "    return comment\n",
    "\n",
    "df_dev = xmlToDf(\"data/train.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserializeDf(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "# df_dev = deserializeDf('data/df_train_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with movie 262929\n",
      "Error with movie 269431\n",
      "Error with movie 257053\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# list all unique movies values\n",
    "movies = df_dev[\"movie\"].unique()\n",
    "\n",
    "#convert all movies values to int\n",
    "for i in range(len(movies)):\n",
    "    movies[i] = int(movies[i])\n",
    "    \n",
    "# load in a df all movies with json files in movies folder\n",
    "df_movies = pd.DataFrame(columns=[\"name\", \"genre\", \"averageRate\", \"director\"], index=movies)\n",
    "for movie in movies:\n",
    "    try:\n",
    "        with open(\"movies/\" + str(movie) + \".json\", encoding=\"utf-8\") as json_file:\n",
    "            data = json.load(json_file, strict=False)\n",
    "            # convert ratingValue to float by replacing comma to point and then multiply by 2 minus 1\n",
    "            data['aggregateRating']['ratingValue'] = float(data['aggregateRating']['ratingValue'].replace(\",\", \".\")) * 2 - 1           \n",
    "            # if key director does not exist, we put an empty string\n",
    "            if 'director' not in data:\n",
    "                row = {'name': data['name'], 'genre': data['genre'], 'averageRate': data['aggregateRating']['ratingValue'], 'director': \"\"}\n",
    "            elif type(data['director']) is list:\n",
    "                row = {'name': data['name'], 'genre': data['genre'], 'averageRate': data['aggregateRating']['ratingValue'], 'director': data['director'][0]['name']}\n",
    "            else:\n",
    "                row = {'name': data['name'], 'genre': data['genre'], 'averageRate': data['aggregateRating']['ratingValue'], 'director': data['director']['name']}       \n",
    "            df_movies.loc[movie] = row\n",
    "    except:\n",
    "        print(\"Error with movie \" + str(movie))\n",
    "        row = {'name': \" \", 'genre': \" \", 'averageRate': \" \", 'director': \" \"}\n",
    "        df_movies.loc[movie] = row\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments = df_dev[\"commentaire\"].values.tolist()\n",
    "# for i in range(len(comments)):\n",
    "#     comments[i] = \" \".join(comments[i])\n",
    "rates = df_dev[\"note\"].values.tolist()\n",
    "\n",
    "TOKENIZER = CamembertTokenizer.from_pretrained(\n",
    "    'camembert/camembert-base',\n",
    "    do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '</s>', '<unk>', '<pad>', '<mask>', '<s>NOTUSED', '</s>NOTUSED']\n"
     ]
    }
   ],
   "source": [
    "# Add the special tokens to the tokenizer.\n",
    "special_tokens_dict = {'additional_special_tokens': ['<MOVIE>', '<GENRE>', '<RATE>', '<DIRECTOR>','<NAME>', '<REVIEW>']}\n",
    "\n",
    "#print all special tokens from tokenizer\n",
    "print(TOKENIZER.all_special_tokens)\n",
    "\n",
    "#define method to add special tokens to comment of a df row by looking for information in df_movies\n",
    "def addSpecialTokens(row):\n",
    "    try:\n",
    "        movie = row[\"movie\"]\n",
    "        genre = df_movies.loc[int(movie)][\"genre\"]\n",
    "        rate = df_movies.loc[int(movie)][\"averageRate\"]\n",
    "        director = df_movies.loc[int(movie)][\"director\"]\n",
    "        name = df_movies.loc[int(movie)][\"name\"]\n",
    "        comment = row[\"commentaire\"]\n",
    "        if type(genre) is list:\n",
    "            genre = \";\".join(genre)\n",
    "        if type(row[\"name\"]) is not str:\n",
    "            return \"<FILM>\" + name + \"<GENRE>\" + genre + \"<NOTE_MOYENNE>\" + str(rate) + \"<REALISATEUR>\" + director + \"<COMMENTAIRE>\" + comment\n",
    "        elif name == \" \":\n",
    "            return \"<UTILISATEUR>\" + row[\"name\"] + \"<COMMENTAIRE>\" + comment\n",
    "        else:   \n",
    "            return \"<FILM>\" + name + \"<GENRE>\" + genre + \"<NOTE_MOYENNE>\" + str(rate) + \"<REALISATEUR>\" + director + \"<UTILISATEUR>\" + row[\"name\"] + \"<COMMENTAIRE>\" + comment\n",
    "    except:\n",
    "        print(\"Error with movie \" + str(row[\"movie\"]))\n",
    "        return \"<UTILISATEUR>\" + row[\"name\"] + \"<COMMENTAIRE>\" + comment\n",
    "#add special tokens to all comments\n",
    "# df_dev[\"commentaire_token\"] = df_dev.apply(addSpecialTokens, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df_dev[\"commentaire\"].values.tolist()\n",
    "\n",
    "# La fonction batch_encode_plus encode un batch de donnees\n",
    "encoded_batch = TOKENIZER.batch_encode_plus(comments,\n",
    "                                            add_special_tokens=True,\n",
    "                                            padding=True,\n",
    "                                            truncation=True,\n",
    "                                            max_length=512,    \n",
    "                                            return_attention_mask = True,\n",
    "                                            return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize encoded_batch\n",
    "with open('data/encoded_batch_train_token.pkl', 'wb') as f:\n",
    "    pickle.dump(encoded_batch, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rates to list of len(rates) arrays with 10 elements with valye 0 or 1 and 1 if rates*2 is equal to index\n",
    "list_rates = []\n",
    "for rate in rates:\n",
    "    # array = [0]*10\n",
    "    # array[int(rate*2)-1] = 1\n",
    "    # list_rates.append(array)\n",
    "    list_rates.append(np.int64(rate*2-1)) # ! perso j ai pas eu besoin d y transformer en one hot (cad array avec que des 0 et un 1 a la bonne classe)\n",
    "\n",
    "rates = torch.tensor(list_rates).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule l'indice qui va delimiter nos datasets d'entrainement et de validation\n",
    "# On utilise 80% du jeu de donnée pour l'entrainement et les 20% restant pour la validation\n",
    "# split_border = int(len(rates)*0.8)\n",
    "\n",
    "# ! Ton dataset de dev est deja le validation dataset donc dans notre cas on a pas besoin de split\n",
    " \n",
    " \n",
    "# train_dataset = TensorDataset(\n",
    "#     encoded_batch['input_ids'][:split_border],\n",
    "#     encoded_batch['attention_mask'][:split_border],\n",
    "#     rates[:split_border])\n",
    "# validation_dataset = TensorDataset(\n",
    "#     encoded_batch['input_ids'][split_border:],\n",
    "#     encoded_batch['attention_mask'][split_border:],\n",
    "#     rates[split_border:])\n",
    "\n",
    "train_dataset = TensorDataset(  # ! sera ton train.xml\n",
    "    encoded_batch['input_ids'],\n",
    "    encoded_batch['attention_mask'],\n",
    "    rates)\n",
    "# validation_dataset = TensorDataset(   # ! sera ton dev.xml\n",
    "#     encoded_batch['input_ids'],\n",
    "#     encoded_batch['attention_mask'],\n",
    "#     rates)\n",
    " \n",
    "# On definit la taille des batchs\n",
    "batch_size = 8\n",
    " \n",
    "# On cree les DataLoaders d'entrainement et de validation\n",
    "# Le dataloader est juste un objet iterable\n",
    "# On le configure pour iterer le jeu d'entrainement de façon aleatoire et creer les batchs.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            # sampler = RandomSampler(train_dataset),\n",
    "            shuffle=True, # ! ca fait pareil que le truc d au dessus mais c'est plus explicite je trouve\n",
    "            batch_size = batch_size)\n",
    " \n",
    "# validation_dataloader = DataLoader(\n",
    "#             validation_dataset,\n",
    "#             sampler = SequentialSampler(validation_dataset),\n",
    "#             batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert/camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# On la version pre-entrainee de camemBERT 'base'\n",
    "model = CamembertForSequenceClassification.from_pretrained('camembert/camembert-base', num_labels = 10).cuda()\n",
    "\n",
    "#add layer to model to predict 10 classes instead of 1\n",
    "# model.classifier = torch.nn.Linear(768, 10).cuda()    # ! perso j ai pas ca\n",
    "\n",
    "# flush df_dev from memory\n",
    "df_dev = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # Learning Rate\n",
    "                  eps = 1e-8) # Epsilon\n",
    "epochs = 1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## Epoch 1 / 1 ##########\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5000/83246 [19:12<5:00:33,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.190\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "# On met le modele sur le GPU\n",
    "device = torch.device('cuda',0)\n",
    " \n",
    "# Pour enregistrer les stats a chaque epoque\n",
    "training_stats = []\n",
    " \n",
    "# Boucle d'entrainement\n",
    "for epoch in range(0, epochs):\n",
    "     \n",
    "    print(\"\")\n",
    "    print(f'########## Epoch {epoch+1} / {epochs} ##########')\n",
    "    print('Training...')\n",
    " \n",
    " \n",
    "    # On initialise la loss pour cette epoque\n",
    "    total_train_loss = 0\n",
    "    total_elem = 0\n",
    "    # On met le modele en mode 'training'\n",
    "    # Dans ce mode certaines couches du modele agissent differement\n",
    "    model.train()    \n",
    " \n",
    "    # Pour chaque batch\n",
    "    for step, (input_id, mask, rate) in enumerate(tqdm(train_dataloader)):\n",
    " \n",
    "        # On fait un print chaque 10 batchs\n",
    "        # if step % 10 == 0 and not step == 0:\n",
    "        #     print(f'  Batch {step} of {len(train_dataloader)}.')\n",
    "         \n",
    "        # On recupere les donnees du batch\n",
    "        input_id,attention_mask, rate  = input_id.to(device), mask.to(device), rate.to(device)\n",
    " \n",
    "        # On met le gradient a 0\n",
    "        model.zero_grad()     \n",
    "        # On passe la donnee au model et on recupere la loss et le logits (sortie avant fonction d'activation)\n",
    "        loss, logits = model(input_id, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=attention_mask, \n",
    "                            labels=rate,\n",
    "                            return_dict=False)\n",
    "        \n",
    "        # On incremente la loss totale\n",
    "        # .item() donne la valeur numerique de la loss\n",
    "        total_train_loss += loss.item()\n",
    "        total_elem += len(rate)\n",
    "        # Backpropagtion\n",
    "        loss.backward()\n",
    "        # loss.backward(retain_graph=True)\n",
    "        # On actualise les parametrer grace a l'optimizer\n",
    "        optimizer.step()\n",
    "        if step>=5000:\n",
    "            break\n",
    " \n",
    "    # On calcule la  loss moyenne sur toute l'epoque\n",
    "    # avg_train_loss = total_train_loss / len(train_dataloader)  # ! len train loader ca donne le nb de batch normalement\n",
    "    # avg_train_loss = total_train_loss / len(train_dataloader.dataset)   # ! .dataset ca donne le nb de donnees au total\n",
    "    avg_train_loss = total_train_loss / total_elem   # ! juste pour debug vu que je fais pas passer tt le dataset\n",
    " \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "     \n",
    "    # Enregistrement des stats de l'epoque\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "        }\n",
    "    )\n",
    " \n",
    "print(\"Model saved!\")\n",
    "# perso je save le model entier\n",
    "# torch.save(model.state_dict(), \"./rates.pt\")\n",
    "# =>\n",
    "torch.save(model, \"model_train_untouch.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54db34dbb873d0124069a1b7e3692f2fcb3af91d00ed8e76b38ddecc02ef7a27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
