{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoModelForSequenceClassification, CamembertForMaskedLM, AutoTokenizer, AutoConfig\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmlToDf(xmlFile):\n",
    "    # Read XML file\n",
    "    df = pd.read_xml(xmlFile)\n",
    "    #replace None to empty string in commentaire column\n",
    "    df[\"commentaire\"] = df[\"commentaire\"].apply(checkIfWordInComment)\n",
    "    #check if not column exist\n",
    "    if \"note\" in df.columns:\n",
    "        # replace comma to point in note column\n",
    "        df[\"note\"] = df[\"note\"].apply(lambda x: x.replace(\",\", \".\"))\n",
    "        # string to double conversion column\n",
    "        df['note'] = df['note'].astype(float)\n",
    "        df['note'] = df['note'].apply(lambda x: x * 2 -1)\n",
    "        df['note'] = df['note'].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def checkIfWordInComment(comment):\n",
    "    if comment is None:\n",
    "        return \"\"\n",
    "    return comment\n",
    "\n",
    "df_train = xmlToDf(\"data/train.xml\")\n",
    "df_dev = xmlToDf(\"data/dev.xml\")\n",
    "df_test = xmlToDf(\"data/test.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to dict\n",
    "dict_train = df_train.to_dict('records')\n",
    "dict_dev = df_dev.to_dict('records')\n",
    "dict_test = df_test.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(samples, tokenizer):\n",
    "    text = [sample[\"commentaire\"] for sample in samples]\n",
    "    \n",
    "    rates = [sample[\"note\"] for sample in samples]\n",
    "    labels = torch.tensor(rates).cuda()\n",
    "    # The tokenizer handles\n",
    "    # - Tokenization (amazing right?)\n",
    "    # - Padding (adding empty tokens so that each example has the same length)\n",
    "    # - Truncation (cutting samples that are too long)\n",
    "    # - Special tokens (in CamemBERT, each sentence ends with a special token </s>)\n",
    "    # - Attention mask (a binary vector which tells the model which tokens to look at. For instance it will not compute anything if the token is a padding token)\n",
    "    tokens = tokenizer.batch_encode_plus( text,\n",
    "                                        add_special_tokens=True,\n",
    "                                        padding=True,\n",
    "                                        truncation=True,\n",
    "                                        max_length=512,\n",
    "                                        return_attention_mask = True,\n",
    "                                        return_tensors = 'pt')\n",
    "\n",
    "    return {\"input_ids\": tokens.input_ids, \"attention_mask\": tokens.attention_mask, \"labels\": labels, \"sentences\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'camembert/camembert-base',\n",
    "    do_lower_case=True)\n",
    "\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dict_dev, \n",
    "    batch_size=8, \n",
    "    shuffle=False,\n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dict_train, \n",
    "    batch_size=8, \n",
    "    shuffle=True, \n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\wiakx\\OneDrive\\1Documents\\Cours\\M2\\INOV2\\Projet\\Projet_camenBERT_clean.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_clean.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mLightningModel\u001b[39;00m(pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_clean.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model_name, num_labels, lr, weight_decay, from_scratch\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_clean.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_labels, lr, weight_decay, from_scratch=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        if from_scratch:\n",
    "            # Si `from_scratch` est vrai, on charge uniquement la config (nombre de couches, hidden size, etc.) et pas les poids du modèle \n",
    "            config = AutoConfig.from_pretrained(\n",
    "                model_name, num_labels=num_labels\n",
    "            )\n",
    "            self.model = AutoModelForSequenceClassification.from_config(config)\n",
    "        else:\n",
    "            # Cette méthode permet de télécharger le bon modèle pré-entraîné directement depuis le Hub de HuggingFace sur lequel sont stockés de nombreux modèles\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_name, num_labels=num_labels\n",
    "            )\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_labels = self.model.num_labels\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        logits = out.logits\n",
    "        # -------- MASKED --------\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits.view(-1, self.num_labels), batch[\"labels\"].view(-1))\n",
    "\n",
    "        # ------ END MASKED ------\n",
    "\n",
    "        self.log(\"train/loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        labels = batch[\"labels\"]\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        preds = torch.max(out.logits, -1).indices\n",
    "        # -------- MASKED --------\n",
    "        acc = (batch[\"labels\"] == preds).float().mean()\n",
    "        # ------ END MASKED ------\n",
    "        self.log(\"valid/acc\", acc)\n",
    "\n",
    "        f1 = f1_score(batch[\"labels\"].cpu().tolist(), preds.cpu().tolist(), average=\"macro\")\n",
    "        self.log(\"valid/f1\", f1)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        \"\"\"La fonction predict step facilite la prédiction de données. Elle est \n",
    "        similaire à `validation_step`, sans le calcul des métriques.\n",
    "        \"\"\"\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        return torch.max(out.logits, -1).indices\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                               | Params\n",
      "-------------------------------------------------------------\n",
      "0 | model | CamembertForSequenceClassification | 110 M \n",
      "-------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.519   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9cd397aa6c492e980ee9448f8dc01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lightning_model = LightningModel(\"camembert-base\", 10, lr=3e-5, weight_decay=0.)\n",
    "model_checkpoint = pl.callbacks.ModelCheckpoint(monitor=\"valid/acc\", mode=\"max\")\n",
    "camembert_trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        pl.callbacks.EarlyStopping(monitor=\"valid/acc\", patience=4, mode=\"max\"),\n",
    "        model_checkpoint,\n",
    "    ]\n",
    ")\n",
    "camembert_trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "lightning_model = LightningModel.load_from_checkpoint(checkpoint_path=model_checkpoint.best_model_path)\n",
    "\n",
    "# save the model\n",
    "torch.save(lightning_model.model.state_dict(), \"model_lightning.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54db34dbb873d0124069a1b7e3692f2fcb3af91d00ed8e76b38ddecc02ef7a27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
