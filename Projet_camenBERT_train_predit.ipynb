{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import seaborn\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmlToDf(xmlFile):\n",
    "    # Read XML file\n",
    "    df = pd.read_xml(xmlFile)\n",
    "    # replace comma to point in note column\n",
    "    df[\"note\"] = df[\"note\"].apply(lambda x: x.replace(\",\", \".\"))\n",
    "    #replace None to empty string in commentaire column\n",
    "    df[\"commentaire\"] = df[\"commentaire\"].apply(checkIfWordInComment)\n",
    "    # string to double conversion column\n",
    "    df['note'] = df['note'].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def checkIfWordInComment(comment):\n",
    "    if comment is None:\n",
    "        return \"\"\n",
    "    return comment\n",
    "\n",
    "df_dev = xmlToDf(\"data/train.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserializeDf(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "# df_dev = deserializeDf('data/df_train_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df_dev[\"commentaire\"].values.tolist()\n",
    "rates = df_dev[\"note\"].values.tolist()\n",
    "\n",
    "TOKENIZER = CamembertTokenizer.from_pretrained(\n",
    "    'camembert/camembert-base',\n",
    "    do_lower_case=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_dev[\"note\"] = df_dev[\"note\"].apply(lambda x: x.replace(\",\", \".\"))\n",
    "# df_dev['note'] = df_dev['note'].astype(float)\n",
    "\n",
    "rates = df_dev[\"note\"].values.tolist()\n",
    "\n",
    "# rates to list of len(rates) arrays with 10 elements with valye 0 or 1 and 1 if rates*2 is equal to index\n",
    "list_rates = []\n",
    "for rate in rates:\n",
    "    # array = [0]*10\n",
    "    # array[int(rate*2)-1] = 1\n",
    "    # list_rates.append(array)\n",
    "    list_rates.append(np.int64(rate*2-1)) # ! perso j ai pas eu besoin d y transformer en one hot (cad array avec que des 0 et un 1 a la bonne classe)\n",
    "\n",
    "rates = torch.tensor(list_rates).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = None\n",
    "\n",
    "# La fonction batch_encode_plus encode un batch de donnees\n",
    "encoded_batch = TOKENIZER.batch_encode_plus(comments,\n",
    "                                            add_special_tokens=True,\n",
    "                                            padding=True,\n",
    "                                            truncation=True,\n",
    "                                            max_length=512,     # ! au lieu de 10 -> 512 car c'est le nb max de token que peut prendre le camembert\n",
    "                                            return_attention_mask = True,\n",
    "                                            return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save encoded_batch\n",
    "with open('data/encoded_batch_train_untouch_comment.pkl', 'wb') as f:\n",
    "    pickle.dump(encoded_batch, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule l'indice qui va delimiter nos datasets d'entrainement et de validation\n",
    "# On utilise 80% du jeu de donnée pour l'entrainement et les 20% restant pour la validation\n",
    "# split_border = int(len(rates)*0.8)\n",
    "\n",
    "# ! Ton dataset de dev est deja le validation dataset donc dans notre cas on a pas besoin de split\n",
    " \n",
    " \n",
    "# train_dataset = TensorDataset(\n",
    "#     encoded_batch['input_ids'][:split_border],\n",
    "#     encoded_batch['attention_mask'][:split_border],\n",
    "#     rates[:split_border])\n",
    "# validation_dataset = TensorDataset(\n",
    "#     encoded_batch['input_ids'][split_border:],\n",
    "#     encoded_batch['attention_mask'][split_border:],\n",
    "#     rates[split_border:])\n",
    "\n",
    "train_dataset = TensorDataset(  # ! sera ton train.xml\n",
    "    encoded_batch['input_ids'],\n",
    "    encoded_batch['attention_mask'],\n",
    "    rates)\n",
    "# validation_dataset = TensorDataset(   # ! sera ton dev.xml\n",
    "#     encoded_batch['input_ids'],\n",
    "#     encoded_batch['attention_mask'],\n",
    "#     rates)\n",
    " \n",
    "# On definit la taille des batchs\n",
    "batch_size = 8\n",
    " \n",
    "# On cree les DataLoaders d'entrainement et de validation\n",
    "# Le dataloader est juste un objet iterable\n",
    "# On le configure pour iterer le jeu d'entrainement de façon aleatoire et creer les batchs.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            # sampler = RandomSampler(train_dataset),\n",
    "            shuffle=True, # ! ca fait pareil que le truc d au dessus mais c'est plus explicite je trouve\n",
    "            batch_size = batch_size)\n",
    " \n",
    "# validation_dataloader = DataLoader(\n",
    "#             validation_dataset,\n",
    "#             sampler = SequentialSampler(validation_dataset),\n",
    "#             batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert/camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# On la version pre-entrainee de camemBERT 'base'\n",
    "model = CamembertForSequenceClassification.from_pretrained('camembert/camembert-base', num_labels = 10).cuda()\n",
    "\n",
    "df_dev = None\n",
    "\n",
    "#add layer to model to predict 10 classes instead of 1\n",
    "# model.classifier = torch.nn.Linear(768, 10).cuda()    # ! perso j ai pas ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # Learning Rate\n",
    "                  eps = 1e-8) # Epsilon\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## Epoch 1 / 1 ##########\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/83246 [00:27<6:22:49,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.281\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "# On met le modele sur le GPU\n",
    "device = torch.device('cuda',0)\n",
    " \n",
    "# Pour enregistrer les stats a chaque epoque\n",
    "training_stats = []\n",
    " \n",
    "# Boucle d'entrainement\n",
    "for epoch in range(0, epochs):\n",
    "     \n",
    "    print(\"\")\n",
    "    print(f'########## Epoch {epoch+1} / {epochs} ##########')\n",
    "    print('Training...')\n",
    " \n",
    " \n",
    "    # On initialise la loss pour cette epoque\n",
    "    total_train_loss = 0\n",
    "    total_elem = 0\n",
    "    # On met le modele en mode 'training'\n",
    "    # Dans ce mode certaines couches du modele agissent differement\n",
    "    model.train()    \n",
    " \n",
    "    # Pour chaque batch\n",
    "    for step, (input_id, mask, rate) in enumerate(tqdm(train_dataloader)):\n",
    " \n",
    "        # On fait un print chaque 10 batchs\n",
    "        # if step % 10 == 0 and not step == 0:\n",
    "        #     print(f'  Batch {step} of {len(train_dataloader)}.')\n",
    "         \n",
    "        # On recupere les donnees du batch\n",
    "        input_id,attention_mask, rate  = input_id.to(device), mask.to(device), rate.to(device)\n",
    " \n",
    "        # On met le gradient a 0\n",
    "        model.zero_grad()     \n",
    "        # On passe la donnee au model et on recupere la loss et le logits (sortie avant fonction d'activation)\n",
    "        loss, logits = model(input_id, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=attention_mask, \n",
    "                            labels=rate,\n",
    "                            return_dict=False)\n",
    "        \n",
    "        # On incremente la loss totale\n",
    "        # .item() donne la valeur numerique de la loss\n",
    "        total_train_loss += loss.item()\n",
    "        total_elem += len(rate)\n",
    "        # Backpropagtion\n",
    "        loss.backward()\n",
    "        # loss.backward(retain_graph=True)\n",
    "        # On actualise les parametrer grace a l'optimizer\n",
    "        optimizer.step()\n",
    "        if step>=100:\n",
    "            break\n",
    " \n",
    "    # On calcule la  loss moyenne sur toute l'epoque\n",
    "    # avg_train_loss = total_train_loss / len(train_dataloader)  # ! len train loader ca donne le nb de batch normalement\n",
    "    # avg_train_loss = total_train_loss / len(train_dataloader.dataset)   # ! .dataset ca donne le nb de donnees au total\n",
    "    avg_train_loss = total_train_loss / total_elem   # ! juste pour debug vu que je fais pas passer tt le dataset\n",
    " \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "     \n",
    "    # Enregistrement des stats de l'epoque\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "        }\n",
    "    )\n",
    " \n",
    "print(\"Model saved!\")\n",
    "# perso je save le model entier\n",
    "# torch.save(model.state_dict(), \"./rates.pt\")\n",
    "# =>\n",
    "torch.save(model, \"model_train_untouched_comments.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 11.72 GiB (GPU 0; 10.00 GiB total capacity; 9.21 GiB already allocated; 0 bytes free; 9.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\wiakx\\OneDrive\\1Documents\\Cours\\M2\\INOV2\\Projet\\Projet_camenBERT_verif.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_verif.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     seaborn\u001b[39m.\u001b[39mheatmap(metrics\u001b[39m.\u001b[39mconfusion_matrix(rates\u001b[39m.\u001b[39mcpu(), predictions\u001b[39m.\u001b[39mcpu()))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_verif.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# predictions = predict(comments, model)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_verif.ipynb#X12sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# print(predictions)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_verif.ipynb#X12sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m evaluate(model, comments[\u001b[39m0\u001b[39;49m:\u001b[39m1000\u001b[39;49m], rates[\u001b[39m0\u001b[39;49m:\u001b[39m1000\u001b[39;49m])\n",
      "\u001b[1;32mc:\\Users\\wiakx\\OneDrive\\1Documents\\Cours\\M2\\INOV2\\Projet\\Projet_camenBERT_verif.ipynb Cell 11\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, reviews, rates)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_verif.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(model, reviews, rates):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_verif.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     predictions \u001b[39m=\u001b[39m predict(reviews, model)\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_verif.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mprint\u001b[39m(predictions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_verif.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mprint\u001b[39m(metrics\u001b[39m.\u001b[39mf1_score(rates\u001b[39m.\u001b[39mcpu(), predictions\u001b[39m.\u001b[39mcpu(), average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m'\u001b[39m, zero_division\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n",
      "\u001b[1;32mc:\\Users\\wiakx\\OneDrive\\1Documents\\Cours\\M2\\INOV2\\Projet\\Projet_camenBERT_verif.ipynb Cell 11\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(reviews, model)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_verif.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m input_ids, attention_mask \u001b[39m=\u001b[39m preprocess(reviews)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_verif.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m input_ids,attention_mask \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mcuda(), attention_mask\u001b[39m.\u001b[39mcuda()  \u001b[39m# ! pour tt mettre sur cuda pour eviter les pb de devices different\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_verif.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m retour \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wiakx/OneDrive/1Documents/Cours/M2/INOV2/Projet/Projet_camenBERT_verif.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39margmax(retour[\u001b[39m0\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1206\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1200\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1202\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1206\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m   1207\u001b[0m     input_ids,\n\u001b[0;32m   1208\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1209\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1210\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1211\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1212\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1213\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1214\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1215\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1216\u001b[0m )\n\u001b[0;32m   1217\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1218\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:848\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    839\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    841\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    842\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    843\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    846\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    847\u001b[0m )\n\u001b[1;32m--> 848\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    849\u001b[0m     embedding_output,\n\u001b[0;32m    850\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    851\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    852\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    853\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    854\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    855\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    856\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    857\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    858\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    859\u001b[0m )\n\u001b[0;32m    860\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    861\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    515\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    516\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    517\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    522\u001b[0m     )\n\u001b[0;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    525\u001b[0m         hidden_states,\n\u001b[0;32m    526\u001b[0m         attention_mask,\n\u001b[0;32m    527\u001b[0m         layer_head_mask,\n\u001b[0;32m    528\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    529\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    530\u001b[0m         past_key_value,\n\u001b[0;32m    531\u001b[0m         output_attentions,\n\u001b[0;32m    532\u001b[0m     )\n\u001b[0;32m    534\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    535\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:409\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    398\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    399\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    406\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    407\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    410\u001b[0m         hidden_states,\n\u001b[0;32m    411\u001b[0m         attention_mask,\n\u001b[0;32m    412\u001b[0m         head_mask,\n\u001b[0;32m    413\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    414\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    415\u001b[0m     )\n\u001b[0;32m    416\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    418\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:336\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    327\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    328\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    334\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    335\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 336\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    337\u001b[0m         hidden_states,\n\u001b[0;32m    338\u001b[0m         attention_mask,\n\u001b[0;32m    339\u001b[0m         head_mask,\n\u001b[0;32m    340\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    341\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    342\u001b[0m         past_key_value,\n\u001b[0;32m    343\u001b[0m         output_attentions,\n\u001b[0;32m    344\u001b[0m     )\n\u001b[0;32m    345\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    346\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Softwares\\Anaconda\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:238\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    235\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[0;32m    237\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    241\u001b[0m     seq_length \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 11.72 GiB (GPU 0; 10.00 GiB total capacity; 9.21 GiB already allocated; 0 bytes free; 9.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# model = CamembertForSequenceClassification.from_pretrained('camembert/camembert-base-oscar-4gb', num_labels = 10).cuda()\n",
    "model = torch.load(\"model.pt\").cuda()   # ! perso je load les modeles comme ca\n",
    "# model.load_state_dict(torch.load(\"./rates.pt\"))\n",
    "model.eval()\n",
    "def preprocess(raw_reviews, rates=None):\n",
    "    encoded_batch = TOKENIZER.batch_encode_plus(raw_reviews,\n",
    "                                                add_special_tokens=True,\n",
    "                                                padding=True,\n",
    "                                                truncation=True,\n",
    "                                                max_length=512, # ! meme raison qu en haut\n",
    "                                                return_attention_mask = True,\n",
    "                                                return_tensors = 'pt')\n",
    "    if rates:\n",
    "        rates = torch.tensor(rates)\n",
    "        return encoded_batch['input_ids'], encoded_batch['attention_mask'], rates\n",
    "    return encoded_batch['input_ids'], encoded_batch['attention_mask']\n",
    " \n",
    "def predict(reviews, model=model):\n",
    "    with torch.no_grad():\n",
    "        input_ids, attention_mask = preprocess(reviews)\n",
    "        input_ids,attention_mask = input_ids.cuda(), attention_mask.cuda()  # ! pour tt mettre sur cuda pour eviter les pb de devices different\n",
    "        retour = model(input_ids, attention_mask=attention_mask)\n",
    "        return torch.argmax(retour[0], dim=1).cuda() # ! dim 1 plutot non?\n",
    " \n",
    " \n",
    "def evaluate(model, reviews, rates):\n",
    "    predictions = predict(reviews, model).cpu()\n",
    "    print(predictions)\n",
    "    print(metrics.f1_score(rates.cpu(), predictions.cpu(), average='weighted', zero_division=0))\n",
    "    seaborn.heatmap(metrics.confusion_matrix(rates.cpu(), predictions.cpu()))\n",
    "\n",
    "# predictions = predict(comments, model)\n",
    "# print(predictions)\n",
    "evaluate(model, comments[0:20], rates[0:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54db34dbb873d0124069a1b7e3692f2fcb3af91d00ed8e76b38ddecc02ef7a27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
