{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Processing des données avec stemming, suppression de stop-words et selection via DF/IDF"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import pickle\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from tqdm.gui import tqdm as tqdm_gui"]},{"cell_type":"markdown","metadata":{},"source":["Import des data via deserialize"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def xmlToDf(xmlFile):\n","    # Read XML file\n","    df = pd.read_xml(xmlFile)\n","    #replace None to empty string in commentaire column\n","    df[\"commentaire\"] = df[\"commentaire\"].apply(checkIfWordInComment)\n","    return df\n","\n","\n","def checkIfWordInComment(comment):\n","    if comment is None:\n","        return \"\"\n","    return comment\n","\n","df_dev_fast = xmlToDf(\"data/dev.xml\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# def deserializeDf(path):\n","#     with open(path, 'rb') as f:\n","#         return pickle.load(f)\n","    \n","# # df_dev = deserializeDf('data/df_dev.pkl')\n","# # df_idf = deserializeDf('data/df_idf.pkl')\n","# df_dev_fast = deserializeDf('data/df_dev_fast.pkl')\n","# #df_idf_dev_fast = deserializeDf('data/df_idf_dev_fast.pkl')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\wiakx\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["\n","import nltk\n","from nltk.corpus import stopwords\n","import spacy\n","\n","nltk.download('stopwords')\n","stopWords = set(stopwords.words('french'))\n","spacy.prefer_gpu()\n","# nlp = spacy.load(\"fr_dep_news_trf\") # less efficient but more accurate\n","nlp = spacy.load(\"fr_core_news_sm\") # more efficient but less accurate\n","\n","stop_words = [\" \", \"l'\", \"l’\", \"la\", \"le\", \"les\", \"d’\", \"d'\", \"de\", \"du\", \"des\", \"une\", \"un\",\n","                \"ce\", \"ces\", \"je\", \"moi\", \"mon\", \"me\", \"mes\", \"tu\", \"toi\", \"ton\", \"te\", \"tes\", \n","                \"il\", \"lui\", \"son\", \"se\", \"ses\", \"nous\", \"notre\", \"nos\", \"vous\", \"votre\", \"vos\",\n","                \"ils\", \"leur\", \"leurs\", \"n'\", \"ne\", \"tout\", \"être\", \"avoir\", \"deja\", \"déjà\",\n","                \"ou\" ,\"où\", \"qu’\", \"qu'\", \"que\", \"qui\", \"quelle\", \"quel\", \"quelles\", \"quels\", \n","                \".\", \",\", \"...\", \"sur\", \"telle\", \"tel\", \"telles\", \"tels\", \"laquelle\", \"lequel\",\n","                \"laquelles\", \"lequels\", \"simplement\", \"comment\", \"quoi\", \"dont\", \"donc\", \"tant\",\n","                \"jamais\", \"rarement\", \"parfois\", \"souvent\", \"toujours\", \"avec\", \"pour\", \"ici\",\n","                \":\", \"(\", \")\", \"[\", \"]\", \"\\\"\", \"y\", \"et\", \"par\", \"fois\", \"peu\", \"on\", \"cela\",\n","                \"mais\", \"dans\", \"en\", \"à\", \"au\", \"même\", \"là\", \"-\", \"si\", \"comme\", \"aussi\",\n","                \"car\", \"parce\", \"quand\"]\n","\n","stopWords = list(stopWords)\n","stopWords.extend(stop_words)\n","tmp = ' '.join(stopWords)\n","tmp = nlp(tmp)\n","test = [X.lemma_ for X in tmp]\n","stopWords = list(dict.fromkeys(test))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Lemma words: 100%|██████████| 100400/100400 [45:27<00:00, 36.82it/s]   \n"]},{"name":"stdout","output_type":"stream","text":["on commence\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100400/100400 [1:29:55<00:00, 18.61it/s]   \n"]}],"source":["import spacy\n","import nltk\n","import multiprocessing as mp\n","import concurrent.futures\n","\n","\n","spacy.prefer_gpu()\n","nlp = spacy.load(\"fr_core_news_sm\") # more efficient but less accurate\n","# nlp = spacy.load(\"fr_dep_news_trf\") # less efficient but more accurate\n","\n","\n","def getLemWord(comment):\n","    if comment is None:\n","        return \"\"\n","    doc = nlp(comment)\n","    tokens = [X.lemma_ for X in doc]\n","    clean_words = []\n","    for token in tokens:\n","        if token in stopWords :\n","            tokens.remove(token)\n","    return tokens\n","\n","\n","\n","tqdm.pandas(desc=\"Lemma words\")\n","# df_dev_fast['lemma_word'] = df_dev_fast['commentaire'].progress_apply(lambda x: getLemWord(x))\n","\n","comments = df_dev_fast['commentaire'].tolist()\n","print('on commence')\n","with concurrent.futures.ThreadPoolExecutor(max_workers=mp.cpu_count()) as executor:\n","    results = list(tqdm(executor.map(getLemWord, comments), total=len(comments)))\n","\n","\n","df_dev_fast['lemma_word'] = results"]},{"cell_type":"markdown","metadata":{},"source":["Serialize df into file (pour save)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def serializeDf(df, path):\n","    with open(path, 'wb') as f:\n","        pickle.dump(df, f)\n","        \n","serializeDf(df_dev_fast, 'data/df_dev_fast.pkl')\n","#serializeDf(df_idf_dev_fast, 'data/df_idf_dev_fast.pkl')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def removeCommaInDF():\n","    for index, row in df_dev_fast.iterrows():\n","        if row['lemma_word'].count(',') > 0:\n","            row['lemma_word'].remove(',')\n","\n","removeCommaInDF()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 100400/100400 [00:00<00:00, 255291.00it/s]\n"]}],"source":["def listToString(s):  \n","    str1 = \" \" \n","    return (str1.join(s))\n","\n","lemma_word_list = df_dev_fast['lemma_word'].tolist()\n","\n","with concurrent.futures.ThreadPoolExecutor(max_workers=mp.cpu_count()) as executor:\n","    results = list(tqdm(executor.map(listToString, lemma_word_list), total=len(lemma_word_list)))\n","\n","df_dev_fast['lemma_word_string'] = results"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idf_lemma_weights</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>le</th>\n","      <td>1.199701</td>\n","    </tr>\n","    <tr>\n","      <th>film</th>\n","      <td>1.283584</td>\n","    </tr>\n","    <tr>\n","      <th>un</th>\n","      <td>1.390312</td>\n","    </tr>\n","    <tr>\n","      <th>ce</th>\n","      <td>1.658640</td>\n","    </tr>\n","    <tr>\n","      <th>être</th>\n","      <td>1.796108</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>majs</th>\n","      <td>11.823780</td>\n","    </tr>\n","    <tr>\n","      <th>majuscules</th>\n","      <td>11.823780</td>\n","    </tr>\n","    <tr>\n","      <th>makabé</th>\n","      <td>11.823780</td>\n","    </tr>\n","    <tr>\n","      <th>majorem</th>\n","      <td>11.823780</td>\n","    </tr>\n","    <tr>\n","      <th>黄海</th>\n","      <td>11.823780</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>79931 rows × 1 columns</p>\n","</div>"],"text/plain":["            idf_lemma_weights\n","le                   1.199701\n","film                 1.283584\n","un                   1.390312\n","ce                   1.658640\n","être                 1.796108\n","...                       ...\n","majs                11.823780\n","majuscules          11.823780\n","makabé              11.823780\n","majorem             11.823780\n","黄海                  11.823780\n","\n","[79931 rows x 1 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#instantiate CountVectorizer() \n","cv=CountVectorizer() \n","# this steps generates word counts for the words in your docs \n","\n","word_count_vector=cv.fit_transform(df_dev_fast['lemma_word_string'])\n","tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n","tfidf_transformer.fit(word_count_vector)\n","# print idf values \n","df_idf_dev_fast = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(),columns=[\"idf_lemma_weights\"]) \n","# sort ascending \n","df_idf_dev_fast.sort_values(by=['idf_lemma_weights'])"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def serializeDf(df, path):\n","    with open(path, 'wb') as f:\n","        pickle.dump(df, f)\n","        \n","serializeDf(df_dev_fast, 'data/df_dev_fast.pkl')\n","serializeDf(df_idf_dev_fast, 'data/df_idf_dev_fast.pkl')"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from multiprocessing import Pool\n","from tqdm.contrib.concurrent import process_map\n","\n","def lemmaWordLow(lemma_word):\n","    if lemma_word is None:\n","        return []\n","    clean_words = []\n","    for token in lemma_word:\n","        if token in df_idf_dev_fast.index and df_idf_dev_fast.loc[token]['idf_lemma_weights'] < 10:\n","            clean_words.append(token)\n","    return clean_words\n","\n","def lemmaWordSuperLow(lemma_word):\n","    if lemma_word is None:\n","        return []\n","    clean_words = []\n","    for token in lemma_word:\n","        if token in df_idf_dev_fast.index and df_idf_dev_fast.loc[token]['idf_lemma_weights'] < 9:\n","            clean_words.append(token)\n","    return clean_words\n","\n","def removeUselessWords():\n","    df_dev_fast['lemma_word_low'] = df_dev_fast['lemma_word'].apply(lambda x: lemmaWordLow(x))\n","\n","def removeSuperUselessWords():\n","    tqdm.pandas(desc=\"Deletings useless words\")\n","    df_dev_fast['lemma_word_super_low'] = df_dev_fast['lemma_word'].progress_apply(lambda x: lemmaWordSuperLow(x))\n","\n","removeUselessWords()\n","# removeSuperUselessWords()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def serializeDf(df, path):\n","    with open(path, 'wb') as f:\n","        pickle.dump(df, f)\n","        \n","serializeDf(df_dev_fast, 'data/df_dev_fast.pkl')\n","serializeDf(df_idf_dev_fast, 'data/df_idf_dev_fast.pkl')"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"54db34dbb873d0124069a1b7e3692f2fcb3af91d00ed8e76b38ddecc02ef7a27"}}},"nbformat":4,"nbformat_minor":2}
