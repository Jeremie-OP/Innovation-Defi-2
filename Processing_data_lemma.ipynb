{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import pickle\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from tqdm.gui import tqdm as tqdm_gui"]},{"cell_type":"markdown","metadata":{},"source":["Import des data via deserialize"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def xmlToDf(xmlFile):\n","    # Read XML file\n","    df = pd.read_xml(xmlFile)\n","    #replace None to empty string in commentaire column\n","    df[\"commentaire\"] = df[\"commentaire\"].apply(checkIfWordInComment)\n","    return df\n","\n","\n","def checkIfWordInComment(comment):\n","    if comment is None:\n","        return \"\"\n","    return comment\n","\n","df_test = xmlToDf(\"data/test.xml\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# def deserializeDf(path):\n","#     with open(path, 'rb') as f:\n","#         return pickle.load(f)\n","    \n","# # df_dev = deserializeDf('data/df_dev.pkl')\n","# # df_idf = deserializeDf('data/df_idf.pkl')\n","# df_test = deserializeDf('data/df_test.pkl')\n","# #df_idf_test = deserializeDf('data/df_idf_test.pkl')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\Miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\wiakx\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["\n","import nltk\n","from nltk.corpus import stopwords\n","import spacy\n","\n","nltk.download('stopwords')\n","stopWords = set(stopwords.words('french'))\n","spacy.prefer_gpu()\n","# nlp = spacy.load(\"fr_dep_news_trf\") # less efficient but more accurate\n","nlp = spacy.load(\"fr_core_news_sm\") # more efficient but less accurate\n","\n","stop_words = [\" \", \"l'\", \"l’\", \"la\", \"le\", \"les\", \"d’\", \"d'\", \"de\", \"du\", \"des\", \"une\", \"un\",\n","                \"ce\", \"ces\", \"je\", \"moi\", \"mon\", \"me\", \"mes\", \"tu\", \"toi\", \"ton\", \"te\", \"tes\", \n","                \"il\", \"lui\", \"son\", \"se\", \"ses\", \"nous\", \"notre\", \"nos\", \"vous\", \"votre\", \"vos\",\n","                \"ils\", \"leur\", \"leurs\", \"n'\", \"ne\", \"tout\", \"être\", \"avoir\", \"deja\", \"déjà\",\n","                \"ou\" ,\"où\", \"qu’\", \"qu'\", \"que\", \"qui\", \"quelle\", \"quel\", \"quelles\", \"quels\", \n","                \".\", \",\", \"...\", \"sur\", \"telle\", \"tel\", \"telles\", \"tels\", \"laquelle\", \"lequel\",\n","                \"laquelles\", \"lequels\", \"simplement\", \"comment\", \"quoi\", \"dont\", \"donc\", \"tant\",\n","                \"jamais\", \"rarement\", \"parfois\", \"souvent\", \"toujours\", \"avec\", \"pour\", \"ici\",\n","                \":\", \"(\", \")\", \"[\", \"]\", \"\\\"\", \"y\", \"et\", \"par\", \"fois\", \"peu\", \"on\", \"cela\",\n","                \"mais\", \"dans\", \"en\", \"à\", \"au\", \"même\", \"là\", \"-\", \"si\", \"comme\", \"aussi\",\n","                \"car\", \"parce\", \"quand\"]\n","\n","stopWords = list(stopWords)\n","stopWords.extend(stop_words)\n","tmp = ' '.join(stopWords)\n","tmp = nlp(tmp)\n","test = [X.lemma_ for X in tmp]\n","stopWords = list(dict.fromkeys(test))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["on commence\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 85847/85847 [41:58<00:00, 34.09it/s]   \n"]}],"source":["import spacy\n","import nltk\n","import multiprocessing as mp\n","import concurrent.futures\n","\n","\n","spacy.prefer_gpu()\n","nlp = spacy.load(\"fr_core_news_sm\") # more efficient but less accurate\n","# nlp = spacy.load(\"fr_dep_news_trf\") # less efficient but more accurate\n","\n","\n","def getLemWord(comment):\n","    if comment is None:\n","        return \"\"\n","    doc = nlp(comment)\n","    tokens = [X.lemma_ for X in doc]\n","    clean_words = []\n","    for token in tokens:\n","        if token in stopWords :\n","            tokens.remove(token)\n","    return tokens\n","\n","\n","\n","tqdm.pandas(desc=\"Lemma words\")\n","# df_test['lemma_word'] = df_test['commentaire'].progress_apply(lambda x: getLemWord(x))\n","\n","comments = df_test['commentaire'].tolist()\n","print('on commence')\n","with concurrent.futures.ThreadPoolExecutor(max_workers=mp.cpu_count()) as executor:\n","    results = list(tqdm(executor.map(getLemWord, comments), total=len(comments)))\n","\n","\n","df_test['lemma_word'] = results"]},{"cell_type":"markdown","metadata":{},"source":["Serialize df into file (pour save)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def serializeDf(df, path):\n","    with open(path, 'wb') as f:\n","        pickle.dump(df, f)\n","        \n","serializeDf(df_test, 'data/df_test_new.pkl')\n","#serializeDf(df_idf_test, 'data/df_idf_test.pkl')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def removeCommaInDF():\n","    for index, row in df_test.iterrows():\n","        if row['lemma_word'].count(',') > 0:\n","            row['lemma_word'].remove(',')\n","\n","removeCommaInDF()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 85847/85847 [00:00<00:00, 344774.89it/s]\n"]}],"source":["def listToString(s):  \n","    str1 = \" \" \n","    return (str1.join(s))\n","\n","lemma_word_list = df_test['lemma_word'].tolist()\n","\n","with concurrent.futures.ThreadPoolExecutor(max_workers=mp.cpu_count()) as executor:\n","    results = list(tqdm(executor.map(listToString, lemma_word_list), total=len(lemma_word_list)))\n","\n","df_test['lemma_word_string'] = results"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idf_lemma_weights</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>le</th>\n","      <td>1.195279</td>\n","    </tr>\n","    <tr>\n","      <th>film</th>\n","      <td>1.286115</td>\n","    </tr>\n","    <tr>\n","      <th>un</th>\n","      <td>1.375940</td>\n","    </tr>\n","    <tr>\n","      <th>ce</th>\n","      <td>1.659114</td>\n","    </tr>\n","    <tr>\n","      <th>être</th>\n","      <td>1.797954</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>inassouvie</th>\n","      <td>11.667186</td>\n","    </tr>\n","    <tr>\n","      <th>inattendues</th>\n","      <td>11.667186</td>\n","    </tr>\n","    <tr>\n","      <th>inauguration</th>\n","      <td>11.667186</td>\n","    </tr>\n","    <tr>\n","      <th>inarpçu</th>\n","      <td>11.667186</td>\n","    </tr>\n","    <tr>\n","      <th>靠视觉和音效吓唬人的时代结束了</th>\n","      <td>11.667186</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>73916 rows × 1 columns</p>\n","</div>"],"text/plain":["                 idf_lemma_weights\n","le                        1.195279\n","film                      1.286115\n","un                        1.375940\n","ce                        1.659114\n","être                      1.797954\n","...                            ...\n","inassouvie               11.667186\n","inattendues              11.667186\n","inauguration             11.667186\n","inarpçu                  11.667186\n","靠视觉和音效吓唬人的时代结束了          11.667186\n","\n","[73916 rows x 1 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#instantiate CountVectorizer() \n","cv=CountVectorizer() \n","# this steps generates word counts for the words in your docs \n","\n","word_count_vector=cv.fit_transform(df_test['lemma_word_string'])\n","tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n","tfidf_transformer.fit(word_count_vector)\n","# print idf values \n","df_idf_test = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(),columns=[\"idf_lemma_weights\"]) \n","# sort ascending \n","df_idf_test.sort_values(by=['idf_lemma_weights'])"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def serializeDf(df, path):\n","    with open(path, 'wb') as f:\n","        pickle.dump(df, f)\n","        \n","serializeDf(df_test, 'data/df_test_new.pkl')\n","serializeDf(df_idf_test, 'data/df_idf_test_new.pkl')"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Deletings useless words: 100%|██████████| 85847/85847 [03:07<00:00, 457.96it/s]\n"]}],"source":["from multiprocessing import Pool\n","from tqdm.contrib.concurrent import process_map\n","\n","def lemmaWordLow(lemma_word):\n","    if lemma_word is None:\n","        return []\n","    clean_words = []\n","    for token in lemma_word:\n","        if token in df_idf_test.index and df_idf_test.loc[token]['idf_lemma_weights'] < 10:\n","            clean_words.append(token)\n","    return clean_words\n","\n","def lemmaWordSuperLow(lemma_word):\n","    if lemma_word is None:\n","        return []\n","    clean_words = []\n","    for token in lemma_word:\n","        if token in df_idf_test.index and df_idf_test.loc[token]['idf_lemma_weights'] < 9:\n","            clean_words.append(token)\n","    return clean_words\n","\n","def removeUselessWords():\n","    tqdm.pandas(desc=\"Deletings useless words\")\n","    df_test['lemma_word_low'] = df_test['lemma_word'].progress_apply(lambda x: lemmaWordLow(x))\n","\n","def removeSuperUselessWords():\n","    tqdm.pandas(desc=\"Deletings useless words\")\n","    df_test['lemma_word_super_low'] = df_test['lemma_word'].progress_apply(lambda x: lemmaWordSuperLow(x))\n","\n","removeUselessWords()\n","# removeSuperUselessWords()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def serializeDf(df, path):\n","    with open(path, 'wb') as f:\n","        pickle.dump(df, f)\n","        \n","serializeDf(df_test, 'data/df_test_new.pkl')\n","serializeDf(df_idf_test, 'data/df_idf_test_new.pkl')"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"54db34dbb873d0124069a1b7e3692f2fcb3af91d00ed8e76b38ddecc02ef7a27"}}},"nbformat":4,"nbformat_minor":2}
